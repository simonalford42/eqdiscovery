{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One pair of (X,y) looks like:\n",
    "- d = 3 # number of dimensions\n",
    "- N # number of objects\n",
    "- X: N x (2 * d + 1) # positions, velocities, mass\n",
    "- y: N x d # accelerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048 * 16\n",
    "visible = 10\n",
    "hidden = 1\n",
    "N = visible + hidden\n",
    "d = 3\n",
    "\n",
    "scale_exp = 5\n",
    "\n",
    "pos = torch.exp(scale_exp * torch.rand(batch_size, N, d))\n",
    "# make it centered at 0\n",
    "pos -= pos.mean(axis=1, keepdim = True) \n",
    "\n",
    "vel = torch.exp(scale_exp * torch.rand(batch_size, N, d))\n",
    "\n",
    "# assign fixed positions, velocities??? (this shouldn't matter for now) to hidden objects (this only works for one that is put in the center for now)\n",
    "pos[:,:hidden,:] *= 0\n",
    "vel[:,:hidden,:] *= 0\n",
    "\n",
    "m = torch.rand(1, N, 1)\n",
    "# hidden mass:\n",
    "m[0,:hidden,0] = m[0,:hidden,0] * 0 + 1\n",
    "\n",
    "m = torch.exp(scale_exp * m)\n",
    "m = m.expand(batch_size,-1,-1)\n",
    "\n",
    "dt = 0.01\n",
    "steps = 10\n",
    "g = 0.5\n",
    "\n",
    "ms = m.unsqueeze(2).expand(-1,-1,N,-1)\n",
    "m1 = ms\n",
    "m2 = ms.transpose(1,2)\n",
    "\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "X_list.append(torch.cat((pos, vel, m), dim=-1))\n",
    "\n",
    "for _ in range(steps):\n",
    "    xs = pos.unsqueeze(2).expand(-1,-1,N,-1)\n",
    "    x1 = xs\n",
    "    x2 = xs.transpose(1,2)\n",
    "\n",
    "    delta_x = x1 - x2\n",
    "    delta_x_norm = (delta_x ** 2).sum(dim=-1, keepdim=True)**0.5 + 1e-9\n",
    "    forces = -1 * g * m1 * m2 / delta_x_norm ** 2\n",
    "\n",
    "    # the delta_x_norms were offset by a small number to avoid numeric problems\n",
    "    # this is fine, when multiplying by delta_x, the self-self terms are zeroed out\n",
    "    force_vectors = forces * delta_x / delta_x_norm\n",
    "    a = force_vectors.sum(dim=2) / m1[:,:,0,:]\n",
    "\n",
    "    # \n",
    "    #y_list.append(a)\n",
    "\n",
    "    # simple 1 step - could use a more intelligent integrator here.\n",
    "    vel += a * dt\n",
    "    pos += vel * dt\n",
    "\n",
    "    #y_list.append(torch.cat((pos, vel, m), dim=-1))\n",
    "\n",
    "y_list.append(torch.cat((pos, vel, m), dim=-1))\n",
    "\n",
    "X = torch.cat(X_list)\n",
    "y = torch.cat(y_list)\n",
    "\n",
    "# remove hidden objects\n",
    "X = X[:,hidden:,:]\n",
    "y = y[:,hidden:,:]\n",
    "\n",
    "# add some random noise\n",
    "#y += 1e-6 * torch.randn(y.shape) * y.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchdyn in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: torchsde in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from torchdyn) (0.2.5)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from torchdyn) (8.0.4)\n",
      "Requirement already satisfied: pytorch-lightning in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from torchdyn) (1.6.1)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from torchdyn) (6.13.0)\n",
      "Requirement already satisfied: torchcde<0.3.0,>=0.2.3 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from torchdyn) (0.2.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from torchdyn) (1.8.1)\n",
      "Requirement already satisfied: torch<2.0.0,>=1.8.1 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from torchdyn) (1.11.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from torchdyn) (3.5.1)\n",
      "Requirement already satisfied: sklearn in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from torchdyn) (0.0.post1)\n",
      "Requirement already satisfied: poethepoet<0.11.0,>=0.10.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from torchdyn) (0.10.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from torchdyn) (0.12.0)\n",
      "Requirement already satisfied: pastel<0.3.0,>=0.2.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from poethepoet<0.11.0,>=0.10.0->torchdyn) (0.2.1)\n",
      "Requirement already satisfied: tomlkit<1.0.0,>=0.6.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from poethepoet<0.11.0,>=0.10.0->torchdyn) (0.11.6)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from torch<2.0.0,>=1.8.1->torchdyn) (4.2.0)\n",
      "Requirement already satisfied: torchdiffeq>=0.2.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from torchcde<0.3.0,>=0.2.3->torchdyn) (0.2.3)\n",
      "Requirement already satisfied: boltons>=20.2.1 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from torchsde->torchdyn) (21.0.0)\n",
      "Requirement already satisfied: numpy>=1.19.* in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from torchsde->torchdyn) (1.22.3)\n",
      "Requirement already satisfied: trampoline>=0.1.2 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from torchsde->torchdyn) (0.1.2)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipykernel->torchdyn) (1.5.5)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipykernel->torchdyn) (8.2.0)\n",
      "Requirement already satisfied: traitlets>=5.1.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipykernel->torchdyn) (5.1.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipykernel->torchdyn) (0.1.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipykernel->torchdyn) (5.9.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipykernel->torchdyn) (21.3)\n",
      "Requirement already satisfied: debugpy>=1.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipykernel->torchdyn) (1.6.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipykernel->torchdyn) (7.3.0)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipykernel->torchdyn) (6.1)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipywidgets->torchdyn) (3.0.5)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipywidgets->torchdyn) (4.0.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from matplotlib->torchdyn) (4.33.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from matplotlib->torchdyn) (9.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from matplotlib->torchdyn) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from matplotlib->torchdyn) (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from matplotlib->torchdyn) (3.0.8)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from matplotlib->torchdyn) (2.8.2)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from pytorch-lightning->torchdyn) (4.64.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from pytorch-lightning->torchdyn) (6.0)\n",
      "Requirement already satisfied: torchmetrics>=0.4.1 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from pytorch-lightning->torchdyn) (0.8.0)\n",
      "Requirement already satisfied: pyDeprecate<0.4.0,>=0.3.1 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from pytorch-lightning->torchdyn) (0.3.2)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from pytorch-lightning->torchdyn) (2022.3.0)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from pytorch-lightning->torchdyn) (2.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from torchvision->torchdyn) (2.27.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->torchdyn) (3.8.1)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipython>=7.23.1->ipykernel->torchdyn) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipython>=7.23.1->ipykernel->torchdyn) (3.0.29)\n",
      "Requirement already satisfied: stack-data in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipython>=7.23.1->ipykernel->torchdyn) (0.2.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipython>=7.23.1->ipykernel->torchdyn) (0.2.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipython>=7.23.1->ipykernel->torchdyn) (2.12.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipython>=7.23.1->ipykernel->torchdyn) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipython>=7.23.1->ipykernel->torchdyn) (0.18.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipython>=7.23.1->ipykernel->torchdyn) (58.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipython>=7.23.1->ipykernel->torchdyn) (0.4.4)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel->torchdyn) (4.9.2)\n",
      "Requirement already satisfied: pyzmq>=22.3 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel->torchdyn) (22.3.0)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel->torchdyn) (0.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->torchdyn) (1.16.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning->torchdyn) (3.3.6)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning->torchdyn) (1.45.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning->torchdyn) (2.6.6)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning->torchdyn) (0.37.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning->torchdyn) (1.0.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning->torchdyn) (0.4.6)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning->torchdyn) (3.20.0rc2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning->torchdyn) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning->torchdyn) (2.1.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning->torchdyn) (0.6.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from requests->torchvision->torchdyn) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from requests->torchvision->torchdyn) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from requests->torchvision->torchdyn) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from requests->torchvision->torchdyn) (2021.10.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->torchdyn) (5.0.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->torchdyn) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->torchdyn) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning->torchdyn) (1.3.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->torchdyn) (0.8.3)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel->torchdyn) (303)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning->torchdyn) (4.11.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel->torchdyn) (0.2.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->torchdyn) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->torchdyn) (1.3.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->torchdyn) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->torchdyn) (1.7.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->torchdyn) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->torchdyn) (6.0.2)\n",
      "Requirement already satisfied: asttokens in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->torchdyn) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->torchdyn) (0.2.2)\n",
      "Requirement already satisfied: executing in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->torchdyn) (0.8.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning->torchdyn) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->torchdyn) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning->torchdyn) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchdyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdyn.core import NeuralODE\n",
    "from torchdyn.datasets import *\n",
    "from torchdyn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Modern ODE solvers provide guarantees about the growth\n",
    "of approximation error, monitor the level of error, and adapt their evaluation strategy on the fly to\n",
    "achieve the requested level of accuracy. This allows the cost of evaluating a model to scale with\n",
    "problem complexity. After training, accuracy can be reduced for real-time or low-power applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\") #torch.device(\"cuda\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODEModule(pl.LightningModule):\n",
    "    def __init__(self, diff_model):\n",
    "        super(ODEModule, self).__init__()\n",
    "        self.loss = F.mse_loss # torch.log(F.mrse_loss) + angle loss\n",
    "        self.lr = 1e-3\n",
    "        self.wd = 1e-5\n",
    "        self.steps = 1 #steps\n",
    "        self.dt = dt * steps\n",
    "        # relative mean weighted error - this wasn't helpful at all\n",
    "        # self.loss = lambda y_hat, y: ((y_hat - y).abs() / (y.abs() + 1e-8)).mean()\n",
    "\n",
    "        self.diff_model = diff_model#.to(device)\n",
    "        self.ode_model = NeuralODE(self.diff_model, return_t_eval = False, sensitivity='adjoint', solver='rk4', solver_adjoint='dopri5', atol_adjoint=1e-2, rtol_adjoint=1e-2)#.to(device)\n",
    "\n",
    "\n",
    "    #def ode_forward(self, X, dt=0.01):\n",
    "    #    return X + dt * self.diff_model(X)    \n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     #return self.ode_model(x)[-1]\n",
    "    #     #return x + dt * self.diff_model(x)    \n",
    "\n",
    "    #     x_local = []\n",
    "    #     x_local.append(x)\n",
    "    #     for i in range(self.steps):\n",
    "    #         x_local.append(x_local[i] + self.dt * self.diff_model(x_local[i]))\n",
    "        \n",
    "    #     return x_local[-1]\n",
    "    \n",
    "    def forward(self, x, i=None):\n",
    "        #return self.ode_model(x)[-1]\n",
    "        #return x + dt * self.diff_model(x)    \n",
    "        if i is None:\n",
    "            i = self.steps\n",
    "\n",
    "        if i == 0:\n",
    "            return x\n",
    "        \n",
    "        return self.forward(x + self.dt * self.diff_model(x), i-1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_hat = self.forward(X)\n",
    "\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.log('train_loss', loss.item(), on_epoch=True, on_step=False)\n",
    "\n",
    "        # log learning terms\n",
    "        for name, fx in self.diff_model.my_loggers.items():\n",
    "            self.log(name, fx(self.diff_model), on_epoch=True, on_step=False)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_hat = self.forward(X)\n",
    "\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.log('validation_loss', loss.item(), on_epoch=True, on_step=False)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.wd)\n",
    "        return optimizer \n",
    "\n",
    "class GnnLogLinearModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GnnLogLinearModel, self).__init__()\n",
    "        self.input_size = 3 # r, m1, m2\n",
    "        self.output_size = 1\n",
    "        self.formula = torch.nn.Linear(self.input_size, self.output_size) \n",
    "        self.my_loggers = {\n",
    "            'r_exp': lambda s: s.formula.weight[0][0].item(),\n",
    "            'm1_exp': lambda s: s.formula.weight[0][1].item(),\n",
    "            'm2_exp': lambda s: s.formula.weight[0][2].item()\n",
    "        }\n",
    "\n",
    "    def forward(self, X):\n",
    "        N = X.shape[1]\n",
    "        xs = X[:,:,:d].unsqueeze(2).expand(-1,-1,N,-1)\n",
    "        v = X[:,:,d:2*d]\n",
    "        m = X[:,:,-1:]\n",
    "\n",
    "        # safe-guarding for neuralode.\n",
    "        #m = torch.max(m,torch.tensor([1e-8]))\n",
    "        #m = m * 0 + 1\n",
    "        ms = m.unsqueeze(2).expand(-1,-1,N,-1)\n",
    "\n",
    "        x1 = xs\n",
    "        x2 = xs.transpose(1,2)\n",
    "\n",
    "        delta_x = x1 - x2\n",
    "        delta_x_norm = (delta_x ** 2).sum(dim=-1, keepdim=True)**0.5 + 1e-9\n",
    "\n",
    "        m1 = ms\n",
    "        m2 = ms.transpose(1,2)\n",
    "\n",
    "        inp = torch.cat((delta_x_norm, m1, m2), dim=-1)\n",
    "\n",
    "        inp_log = torch.log(inp)\n",
    "\n",
    "        # one linear layer\n",
    "        forces_log = self.formula(inp_log)\n",
    "\n",
    "        forces = torch.exp(forces_log)\n",
    "\n",
    "        # the delta_x_norms were offset by a small number to avoid numeric problems\n",
    "        # this is fine, when multiplying by delta_x, the self-self terms are zeroed out\n",
    "        force_vectors = forces * delta_x / delta_x_norm\n",
    "\n",
    "        a = -1 * force_vectors.sum(dim=2) / m1[:,:,0,:]\n",
    "        \n",
    "        dX = torch.cat((v, a, X[:,:,-1:]*0), dim=-1)\n",
    "\n",
    "        # later learn this directionality too (the -1)\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.to(device)\n",
    "y = y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your vector field callable (nn.Module) should have both time `t` and state `x` as arguments, we've wrapped it for you.\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "Your vector field callable (nn.Module) should have both time `t` and state `x` as arguments, we've wrapped it for you.\n"
     ]
    }
   ],
   "source": [
    "train_set = list(zip(X, y))\n",
    "train_set_size = int(len(train_set) * 0.8)\n",
    "valid_set_size = len(train_set) - train_set_size\n",
    "train_set, valid_set = random_split(train_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "best_model = None\n",
    "best_score = 1e15\n",
    "times = 2\n",
    "\n",
    "for _ in range(times):\n",
    "    diff_model = GnnLogLinearModel()#.to(device)\n",
    "    model = ODEModule(diff_model)#.to(device)\n",
    "    y_hat = model.forward(X)\n",
    "    loss = model.loss(y_hat, y)\n",
    "    if loss < best_score:\n",
    "        print(loss)\n",
    "        best_score = loss\n",
    "        best_model = model\n",
    "\n",
    "\n",
    "model = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass and loss calculation runs okay, without issues\n",
    "y_hat = model.forward(X)\n",
    "loss = model.loss(y_hat, y)\n",
    "# backward pass takes an eternity (even on a batch of 16)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type              | Params\n",
      "-------------------------------------------------\n",
      "0 | diff_model | GnnLogLinearModel | 4     \n",
      "1 | ode_model  | NeuralODE         | 4     \n",
      "-------------------------------------------------\n",
      "4         Trainable params\n",
      "0         Non-trainable params\n",
      "4         Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "c:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:486: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1927: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "early_stop_callback = EarlyStopping(monitor=\"validation_loss\", patience=300, verbose=False, mode=\"min\")\n",
    "\n",
    "train_set = DataLoader(train_set, shuffle=True, batch_size=128)\n",
    "valid_set = DataLoader(valid_set, shuffle=True, batch_size=1000)\n",
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=f'gnn_log_linear_ode') # _masses, hidden_multiple\n",
    "\n",
    "# train with both splits\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=10000, #gpus=1\n",
    "                            #gradient_clip_val=0.5,\n",
    "                            callbacks=[early_stop_callback],\n",
    "                            logger=logger,\n",
    "                            enable_progress_bar=False)\n",
    "\n",
    "trainer.fit(model, train_set, valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your vector field callable (nn.Module) should have both time `t` and state `x` as arguments, we've wrapped it for you.\n",
      "tensor(2.1634, grad_fn=<MseLossBackward0>)\n",
      "Your vector field callable (nn.Module) should have both time `t` and state `x` as arguments, we've wrapped it for you.\n",
      "tensor(0.0333, grad_fn=<MseLossBackward0>)\n",
      "Your vector field callable (nn.Module) should have both time `t` and state `x` as arguments, we've wrapped it for you.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type              | Params\n",
      "-------------------------------------------------\n",
      "0 | diff_model | GnnLogLinearModel | 4     \n",
      "1 | ode_model  | NeuralODE         | 4     \n",
      "-------------------------------------------------\n",
      "4         Trainable params\n",
      "0         Non-trainable params\n",
      "4         Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "c:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:724: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "# not a single epoch in 1 hour\n",
    "\n",
    "\n",
    "\n",
    "for mult in [1]:\n",
    "\n",
    "    train_set = list(zip(X, y))\n",
    "    train_set_size = int(len(train_set) * 0.8)\n",
    "    valid_set_size = len(train_set) - train_set_size\n",
    "    train_set, valid_set = random_split(train_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    best_model = None\n",
    "    best_score = 1e15\n",
    "    times = 3\n",
    "\n",
    "    for _ in range(times):\n",
    "        diff_model = GnnLogLinearModel()#.to(device)\n",
    "        model = ODEModule(diff_model)#.to(device)\n",
    "        y_hat = model.forward(X)\n",
    "        loss = model.loss(y_hat, y)\n",
    "        if loss < best_score:\n",
    "            print(loss)\n",
    "            best_score = loss\n",
    "            best_model = model\n",
    "\n",
    "\n",
    "    model = best_model\n",
    "    model.steps = 2\n",
    "    early_stop_callback = EarlyStopping(monitor=\"validation_loss\", patience=300, verbose=False, mode=\"min\")\n",
    "\n",
    "    train_set = DataLoader(train_set, shuffle=True, batch_size=128)\n",
    "    valid_set = DataLoader(valid_set, shuffle=True, batch_size=1000)\n",
    "\n",
    "    logger = TensorBoardLogger(\"lightning_logs\", name=f'gnn_log_linear_ode_{model.steps}') # _masses, hidden_multiple\n",
    "\n",
    "    # train with both splits\n",
    "    trainer = pl.Trainer(gpus=1, max_epochs=10000, #gpus=1\n",
    "                                #gradient_clip_val=0.5,\n",
    "                                callbacks=[early_stop_callback],\n",
    "                                logger=logger,\n",
    "                                enable_progress_bar=False)\n",
    "\n",
    "    trainer.fit(model, train_set, valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your vector field callable (nn.Module) should have both time `t` and state `x` as arguments, we've wrapped it for you.\n"
     ]
    }
   ],
   "source": [
    "diff_model = GnnLogLinearModel()#.to(device)\n",
    "model = ODEModule(diff_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_local = []\n",
    "x = X[:128]\n",
    "model.steps = 5\n",
    "x_local.append(x)\n",
    "for i in range(model.steps):\n",
    "    x_local.append(x_local[i] + model.dt * model.diff_model(x_local[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_local[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-60.7288), tensor(148.3710))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_local[0].min(), x_local[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-60.3059, grad_fn=<MinBackward1>),\n",
       " tensor(156.5332, grad_fn=<MaxBackward1>))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_local[1].min(), x_local[1].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-74.1496, grad_fn=<MinBackward1>),\n",
       " tensor(163.1830, grad_fn=<MaxBackward1>))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_local[2].min(), x_local[2].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-110.7478, grad_fn=<MinBackward1>),\n",
       " tensor(173.4416, grad_fn=<MaxBackward1>))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_local[3].min(), x_local[3].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: lightning_logs\\gnn_log_linear_ode_4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0.025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type              | Params\n",
      "-------------------------------------------------\n",
      "0 | diff_model | GnnLogLinearModel | 4     \n",
      "1 | ode_model  | NeuralODE         | 4     \n",
      "-------------------------------------------------\n",
      "4         Trainable params\n",
      "0         Non-trainable params\n",
      "4         Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    }
   ],
   "source": [
    "for mult in [1]:\n",
    "\n",
    "    train_set = list(zip(X, y))\n",
    "    train_set_size = int(len(train_set) * 0.8)\n",
    "    valid_set_size = len(train_set) - train_set_size\n",
    "    train_set, valid_set = random_split(train_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    model.steps += 1\n",
    "    model.dt = dt * steps / model.steps\n",
    "    print(model.steps, model.dt)\n",
    "\n",
    "    early_stop_callback = EarlyStopping(monitor=\"validation_loss\", patience=300, verbose=False, mode=\"min\")\n",
    "\n",
    "    train_set = DataLoader(train_set, shuffle=True, batch_size=128)\n",
    "    valid_set = DataLoader(valid_set, shuffle=True, batch_size=1000)\n",
    "\n",
    "    logger = TensorBoardLogger(\"lightning_logs\", name=f'gnn_log_linear_ode_{model.steps}') # _masses, hidden_multiple\n",
    "\n",
    "    # train with both splits\n",
    "    trainer = pl.Trainer(gpus=1, max_epochs=10000, #gpus=1\n",
    "                                #gradient_clip_val=0.5,\n",
    "                                callbacks=[early_stop_callback],\n",
    "                                logger=logger,\n",
    "                                enable_progress_bar=False)\n",
    "\n",
    "    trainer.fit(model, train_set, valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type              | Params\n",
      "-------------------------------------------------\n",
      "0 | diff_model | GnnLogLinearModel | 4     \n",
      "1 | ode_model  | NeuralODE         | 4     \n",
      "-------------------------------------------------\n",
      "4         Trainable params\n",
      "0         Non-trainable params\n",
      "4         Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    }
   ],
   "source": [
    "for mult in [1]:\n",
    "\n",
    "    train_set = list(zip(X, y))\n",
    "    train_set_size = int(len(train_set) * 0.8)\n",
    "    valid_set_size = len(train_set) - train_set_size\n",
    "    train_set, valid_set = random_split(train_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    model.steps = 1\n",
    "    model.dt = dt * steps / model.steps\n",
    "    print(model.steps, model.dt)\n",
    "\n",
    "    early_stop_callback = EarlyStopping(monitor=\"validation_loss\", patience=300, verbose=False, mode=\"min\")\n",
    "\n",
    "    train_set = DataLoader(train_set, shuffle=True, batch_size=128)\n",
    "    valid_set = DataLoader(valid_set, shuffle=True, batch_size=1000)\n",
    "\n",
    "    logger = TensorBoardLogger(\"lightning_logs\", name=f'gnn_log_linear_ode_{model.steps}') # _masses, hidden_multiple\n",
    "\n",
    "    # train with both splits\n",
    "    trainer = pl.Trainer(gpus=1, max_epochs=10000, #gpus=1\n",
    "                                #gradient_clip_val=0.5,\n",
    "                                callbacks=[early_stop_callback],\n",
    "                                logger=logger,\n",
    "                                enable_progress_bar=False)\n",
    "\n",
    "    trainer.fit(model, train_set, valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[nan, nan, nan]], requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.diff_model.formula.weight"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type              | Params\n",
      "-------------------------------------------------\n",
      "0 | diff_model | GnnLogLinearModel | 4     \n",
      "1 | ode_model  | NeuralODE         | 4     \n",
      "-------------------------------------------------\n",
      "4         Trainable params\n",
      "0         Non-trainable params\n",
      "4         Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.07479238510131836,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Sanity Checking",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2d9ae1e07f141c8a91635ebec1af9f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:486: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.028936386108398438,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b76f0df68c44a4a1ee2a8b826d38a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02892446517944336,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "265a6c81dd4649559f4e5be5d370bfd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.07379746437072754,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2458a1e61386487f893a2c63db35895f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:724: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "early_stop_callback = EarlyStopping(monitor=\"validation_loss\", patience=300, verbose=False, mode=\"min\")\n",
    "\n",
    "train_set = list(zip(X, y))\n",
    "train_set = list(zip(X, y))\n",
    "train_set_size = int(len(train_set) * 0.8)\n",
    "valid_set_size = len(train_set) - train_set_size\n",
    "train_set, valid_set = random_split(train_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_set = DataLoader(train_set, shuffle=True, batch_size=128)\n",
    "valid_set = DataLoader(valid_set, shuffle=True, batch_size=1000)\n",
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name='gnn_log_linear_mult1') # _masses, hidden_multiple\n",
    "\n",
    "# train with both splits\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=10000,\n",
    "                            #gradient_clip_val=0.5,\n",
    "                            callbacks=[early_stop_callback],\n",
    "                            logger=logger)\n",
    "\n",
    "trainer.fit(model, train_set, valid_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[40.4088],\n",
       "         [ 2.9725],\n",
       "         [ 5.6881],\n",
       "         [ 8.0561],\n",
       "         [32.0705],\n",
       "         [ 2.0293],\n",
       "         [ 1.0138],\n",
       "         [59.0399],\n",
       "         [60.7772],\n",
       "         [ 3.3891]]),\n",
       " tensor([[[39.0646],\n",
       "          [ 2.9494],\n",
       "          [ 5.6877],\n",
       "          [ 8.0410],\n",
       "          [31.9989],\n",
       "          [ 2.0037],\n",
       "          [ 0.9688],\n",
       "          [58.7323],\n",
       "          [60.7730],\n",
       "          [ 3.3401]]], device='cuda:0', grad_fn=<PowBackward1>))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare fitted masses to ground truth\n",
    "g * m[0],  model.masses ** model.formula.weight[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to run tensorboard in the terminal:\n",
    "# tensorboard --logdir lightning_logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('torch_pl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4741426e26a5e6a637207f4863e4b645de3b3c5f81c70cde841ac5e1e8af37aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
