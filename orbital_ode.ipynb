{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One pair of (X,y) looks like:\n",
    "- d = 3 # number of dimensions\n",
    "- N # number of objects\n",
    "- X: N x (2 * d + 1) # positions, velocities, mass\n",
    "- y: N x d # accelerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048 * 16\n",
    "visible = 10\n",
    "hidden = 1\n",
    "N = visible + hidden\n",
    "d = 3\n",
    "\n",
    "scale_exp = 5\n",
    "\n",
    "pos = torch.exp(scale_exp * torch.rand(batch_size, N, d))\n",
    "# make it centered at 0\n",
    "pos -= pos.mean(axis=1, keepdim = True) \n",
    "\n",
    "vel = torch.exp(scale_exp * torch.rand(batch_size, N, d))\n",
    "\n",
    "# assign fixed positions, velocities??? (this shouldn't matter for now) to hidden objects (this only works for one that is put in the center for now)\n",
    "pos[:,:hidden,:] *= 0\n",
    "vel[:,:hidden,:] *= 0\n",
    "\n",
    "m = torch.rand(1, N, 1)\n",
    "# hidden mass:\n",
    "m[0,:hidden,0] = m[0,:hidden,0] * 0 + 1\n",
    "\n",
    "m = torch.exp(scale_exp * m)\n",
    "m = m.expand(batch_size,-1,-1)\n",
    "\n",
    "dt = 0.01\n",
    "steps = 10\n",
    "g = 0.5\n",
    "\n",
    "ms = m.unsqueeze(2).expand(-1,-1,N,-1)\n",
    "m1 = ms\n",
    "m2 = ms.transpose(1,2)\n",
    "\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "X_list.append(torch.cat((pos, vel, m), dim=-1))\n",
    "\n",
    "for _ in range(steps):\n",
    "    xs = pos.unsqueeze(2).expand(-1,-1,N,-1)\n",
    "    x1 = xs\n",
    "    x2 = xs.transpose(1,2)\n",
    "\n",
    "    delta_x = x1 - x2\n",
    "    delta_x_norm = (delta_x ** 2).sum(dim=-1, keepdim=True)**0.5 + 1e-9\n",
    "    forces = -1 * g * m1 * m2 / delta_x_norm ** 2\n",
    "\n",
    "    # the delta_x_norms were offset by a small number to avoid numeric problems\n",
    "    # this is fine, when multiplying by delta_x, the self-self terms are zeroed out\n",
    "    force_vectors = forces * delta_x / delta_x_norm\n",
    "    a = force_vectors.sum(dim=2) / m1[:,:,0,:]\n",
    "\n",
    "    # \n",
    "    #y_list.append(a)\n",
    "\n",
    "    # simple 1 step - could use a more intelligent integrator here.\n",
    "    vel += a * dt\n",
    "    pos += vel * dt\n",
    "\n",
    "    #y_list.append(torch.cat((pos, vel, m), dim=-1))\n",
    "\n",
    "y_list.append(torch.cat((pos, vel, m), dim=-1))\n",
    "\n",
    "X = torch.cat(X_list)\n",
    "y = torch.cat(y_list)\n",
    "\n",
    "# remove hidden objects\n",
    "X = X[:,hidden:,:]\n",
    "y = y[:,hidden:,:]\n",
    "\n",
    "# add some random noise\n",
    "#y += 1e-6 * torch.randn(y.shape) * y.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchdyn in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: torchsde in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from torchdyn) (0.2.5)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from torchdyn) (8.0.4)\n",
      "Requirement already satisfied: pytorch-lightning in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from torchdyn) (1.6.1)\n",
      "Requirement already satisfied: ipykernel in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from torchdyn) (6.13.0)\n",
      "Requirement already satisfied: torchcde<0.3.0,>=0.2.3 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from torchdyn) (0.2.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from torchdyn) (1.8.1)\n",
      "Requirement already satisfied: torch<2.0.0,>=1.8.1 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from torchdyn) (1.11.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from torchdyn) (3.5.1)\n",
      "Requirement already satisfied: sklearn in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from torchdyn) (0.0.post1)\n",
      "Requirement already satisfied: poethepoet<0.11.0,>=0.10.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from torchdyn) (0.10.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from torchdyn) (0.12.0)\n",
      "Requirement already satisfied: pastel<0.3.0,>=0.2.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from poethepoet<0.11.0,>=0.10.0->torchdyn) (0.2.1)\n",
      "Requirement already satisfied: tomlkit<1.0.0,>=0.6.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from poethepoet<0.11.0,>=0.10.0->torchdyn) (0.11.6)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from torch<2.0.0,>=1.8.1->torchdyn) (4.2.0)\n",
      "Requirement already satisfied: torchdiffeq>=0.2.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from torchcde<0.3.0,>=0.2.3->torchdyn) (0.2.3)\n",
      "Requirement already satisfied: boltons>=20.2.1 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from torchsde->torchdyn) (21.0.0)\n",
      "Requirement already satisfied: numpy>=1.19.* in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from torchsde->torchdyn) (1.22.3)\n",
      "Requirement already satisfied: trampoline>=0.1.2 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from torchsde->torchdyn) (0.1.2)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipykernel->torchdyn) (1.5.5)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipykernel->torchdyn) (8.2.0)\n",
      "Requirement already satisfied: traitlets>=5.1.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipykernel->torchdyn) (5.1.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipykernel->torchdyn) (0.1.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipykernel->torchdyn) (5.9.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipykernel->torchdyn) (21.3)\n",
      "Requirement already satisfied: debugpy>=1.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipykernel->torchdyn) (1.6.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipykernel->torchdyn) (7.3.0)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipykernel->torchdyn) (6.1)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipywidgets->torchdyn) (3.0.5)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipywidgets->torchdyn) (4.0.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from matplotlib->torchdyn) (4.33.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from matplotlib->torchdyn) (9.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from matplotlib->torchdyn) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from matplotlib->torchdyn) (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from matplotlib->torchdyn) (3.0.8)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from matplotlib->torchdyn) (2.8.2)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from pytorch-lightning->torchdyn) (4.64.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from pytorch-lightning->torchdyn) (6.0)\n",
      "Requirement already satisfied: torchmetrics>=0.4.1 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from pytorch-lightning->torchdyn) (0.8.0)\n",
      "Requirement already satisfied: pyDeprecate<0.4.0,>=0.3.1 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from pytorch-lightning->torchdyn) (0.3.2)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from pytorch-lightning->torchdyn) (2022.3.0)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from pytorch-lightning->torchdyn) (2.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from torchvision->torchdyn) (2.27.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->torchdyn) (3.8.1)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipython>=7.23.1->ipykernel->torchdyn) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipython>=7.23.1->ipykernel->torchdyn) (3.0.29)\n",
      "Requirement already satisfied: stack-data in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipython>=7.23.1->ipykernel->torchdyn) (0.2.0)\n",
      "Requirement already satisfied: backcall in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipython>=7.23.1->ipykernel->torchdyn) (0.2.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipython>=7.23.1->ipykernel->torchdyn) (2.12.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipython>=7.23.1->ipykernel->torchdyn) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipython>=7.23.1->ipykernel->torchdyn) (0.18.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipython>=7.23.1->ipykernel->torchdyn) (58.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from ipython>=7.23.1->ipykernel->torchdyn) (0.4.4)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel->torchdyn) (4.9.2)\n",
      "Requirement already satisfied: pyzmq>=22.3 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel->torchdyn) (22.3.0)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel->torchdyn) (0.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->torchdyn) (1.16.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning->torchdyn) (3.3.6)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning->torchdyn) (1.45.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning->torchdyn) (2.6.6)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning->torchdyn) (0.37.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning->torchdyn) (1.0.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning->torchdyn) (0.4.6)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning->torchdyn) (3.20.0rc2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning->torchdyn) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning->torchdyn) (2.1.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from tensorboard>=2.2.0->pytorch-lightning->torchdyn) (0.6.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from requests->torchvision->torchdyn) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from requests->torchvision->torchdyn) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from requests->torchvision->torchdyn) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from requests->torchvision->torchdyn) (2021.10.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->torchdyn) (5.0.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->torchdyn) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->torchdyn) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning->torchdyn) (1.3.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->torchdyn) (0.8.3)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel->torchdyn) (303)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning->torchdyn) (4.11.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel->torchdyn) (0.2.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->torchdyn) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->torchdyn) (1.3.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->torchdyn) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->torchdyn) (1.7.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->torchdyn) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->torchdyn) (6.0.2)\n",
      "Requirement already satisfied: asttokens in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->torchdyn) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->torchdyn) (0.2.2)\n",
      "Requirement already satisfied: executing in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel->torchdyn) (0.8.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning->torchdyn) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->torchdyn) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning->torchdyn) (3.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchdyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdyn.core import NeuralODE\n",
    "from torchdyn.datasets import *\n",
    "from torchdyn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Modern ODE solvers provide guarantees about the growth\n",
    "of approximation error, monitor the level of error, and adapt their evaluation strategy on the fly to\n",
    "achieve the requested level of accuracy. This allows the cost of evaluating a model to scale with\n",
    "problem complexity. After training, accuracy can be reduced for real-time or low-power applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\") #torch.device(\"cuda\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODEModule(pl.LightningModule):\n",
    "    def __init__(self, diff_model):\n",
    "        super(ODEModule, self).__init__()\n",
    "        self.loss = F.mse_loss # torch.log(F.mrse_loss) + angle loss\n",
    "        self.lr = 1e-3\n",
    "        self.wd = 1e-5\n",
    "        self.steps = 1 #steps\n",
    "        self.dt = dt * steps\n",
    "        # relative mean weighted error - this wasn't helpful at all\n",
    "        # self.loss = lambda y_hat, y: ((y_hat - y).abs() / (y.abs() + 1e-8)).mean()\n",
    "\n",
    "        self.diff_model = diff_model#.to(device)\n",
    "        #self.ode_model = NeuralODE(self.diff_model, return_t_eval = False, sensitivity='adjoint', solver='rk4', solver_adjoint='dopri5', atol_adjoint=1e-2, rtol_adjoint=1e-2)#.to(device)\n",
    "\n",
    "\n",
    "    #def ode_forward(self, X, dt=0.01):\n",
    "    #    return X + dt * self.diff_model(X)    \n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     #return self.ode_model(x)[-1]\n",
    "    #     #return x + dt * self.diff_model(x)    \n",
    "\n",
    "    #     x_local = []\n",
    "    #     x_local.append(x)\n",
    "    #     for i in range(self.steps):\n",
    "    #         x_local.append(x_local[i] + self.dt * self.diff_model(x_local[i]))\n",
    "        \n",
    "    #     return x_local[-1]\n",
    "    \n",
    "    def forward(self, x, i=None):\n",
    "        #return self.ode_model(x)[-1]\n",
    "        #return x + dt * self.diff_model(x)    \n",
    "        if i is None:\n",
    "            i = self.steps\n",
    "\n",
    "        if i == 0:\n",
    "            return x\n",
    "        \n",
    "        return self.forward(x + self.dt * self.diff_model(x), i-1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_hat = self.forward(X)\n",
    "\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.log('train_loss', loss.item(), on_epoch=True, on_step=False)\n",
    "\n",
    "        # log learning terms\n",
    "        for name, fx in self.diff_model.my_loggers.items():\n",
    "            self.log(name, fx(self.diff_model), on_epoch=True, on_step=False)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_hat = self.forward(X)\n",
    "\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.log('validation_loss', loss.item(), on_epoch=True, on_step=False)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.wd)\n",
    "        return optimizer \n",
    "\n",
    "class GnnLogLinearModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GnnLogLinearModel, self).__init__()\n",
    "        self.input_size = 3 # r, m1, m2\n",
    "        self.output_size = 1\n",
    "        self.formula = torch.nn.Linear(self.input_size, self.output_size) \n",
    "        self.my_loggers = {\n",
    "            'r_exp': lambda s: s.formula.weight[0][0].item(),\n",
    "            'm1_exp': lambda s: s.formula.weight[0][1].item(),\n",
    "            'm2_exp': lambda s: s.formula.weight[0][2].item()\n",
    "        }\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        N = X.shape[1]\n",
    "        xs = X[:,:,:d].unsqueeze(2).expand(-1,-1,N,-1)\n",
    "        v = X[:,:,d:2*d]\n",
    "        m = X[:,:,-1:]\n",
    "\n",
    "        # safe-guarding for neuralode.\n",
    "        #m = torch.max(m,torch.tensor([1e-8]))\n",
    "        #m = m * 0 + 1\n",
    "        ms = m.unsqueeze(2).expand(-1,-1,N,-1)\n",
    "\n",
    "        x1 = xs\n",
    "        x2 = xs.transpose(1,2)\n",
    "\n",
    "        delta_x = x1 - x2\n",
    "        delta_x_norm = (delta_x ** 2).sum(dim=-1, keepdim=True)**0.5 + 1e-9\n",
    "\n",
    "        m1 = ms\n",
    "        m2 = ms.transpose(1,2)\n",
    "\n",
    "        inp = torch.cat((delta_x_norm, m1, m2), dim=-1)\n",
    "\n",
    "        inp_log = torch.log(inp)\n",
    "\n",
    "        # one linear layer\n",
    "        forces_log = self.formula(inp_log)\n",
    "\n",
    "        forces = torch.exp(forces_log)\n",
    "\n",
    "        # the delta_x_norms were offset by a small number to avoid numeric problems\n",
    "        # this is fine, when multiplying by delta_x, the self-self terms are zeroed out\n",
    "        force_vectors = forces * delta_x / delta_x_norm\n",
    "\n",
    "        a = -1 * force_vectors.sum(dim=2) / m1[:,:,0,:]\n",
    "        \n",
    "        dX = torch.cat((v, a, X[:,:,-1:]*0), dim=-1) * 0 + 1e-6\n",
    "\n",
    "        # later learn this directionality too (the -1)\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.to(device)\n",
    "y = y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.4579, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "train_set = list(zip(X, y))\n",
    "train_set_size = int(len(train_set) * 0.8)\n",
    "valid_set_size = len(train_set) - train_set_size\n",
    "train_set, valid_set = random_split(train_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "best_model = None\n",
    "best_score = 1e15\n",
    "times = 2\n",
    "\n",
    "for _ in range(times):\n",
    "    diff_model = GnnLogLinearModel()#.to(device)\n",
    "    model = ODEModule(diff_model)#.to(device)\n",
    "    y_hat = model.forward(X)\n",
    "    loss = model.loss(y_hat, y)\n",
    "    if loss < best_score:\n",
    "        print(loss)\n",
    "        best_score = loss\n",
    "        best_model = model\n",
    "\n",
    "\n",
    "model = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass and loss calculation runs okay, without issues\n",
    "y_hat = model.forward(X)\n",
    "loss = model.loss(y_hat, y)\n",
    "# backward pass takes an eternity (even on a batch of 16)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type              | Params\n",
      "-------------------------------------------------\n",
      "0 | diff_model | GnnLogLinearModel | 4     \n",
      "1 | ode_model  | NeuralODE         | 4     \n",
      "-------------------------------------------------\n",
      "4         Trainable params\n",
      "0         Non-trainable params\n",
      "4         Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "c:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:486: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1927: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "early_stop_callback = EarlyStopping(monitor=\"validation_loss\", patience=300, verbose=False, mode=\"min\")\n",
    "\n",
    "train_set = DataLoader(train_set, shuffle=True, batch_size=128)\n",
    "valid_set = DataLoader(valid_set, shuffle=True, batch_size=1000)\n",
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=f'gnn_log_linear_ode') # _masses, hidden_multiple\n",
    "\n",
    "# train with both splits\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=10000, #gpus=1\n",
    "                            #gradient_clip_val=0.5,\n",
    "                            callbacks=[early_stop_callback],\n",
    "                            logger=logger,\n",
    "                            enable_progress_bar=False)\n",
    "\n",
    "trainer.fit(model, train_set, valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(25.1815)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type              | Params\n",
      "-------------------------------------------------\n",
      "0 | diff_model | GnnLogLinearModel | 4     \n",
      "-------------------------------------------------\n",
      "4         Trainable params\n",
      "0         Non-trainable params\n",
      "4         Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "c:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:486: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\openSource\\ts_inductive\\orbital_ode.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/openSource/ts_inductive/orbital_ode.ipynb#X16sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m# train with both splits\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/openSource/ts_inductive/orbital_ode.ipynb#X16sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(gpus\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, max_epochs\u001b[39m=\u001b[39m\u001b[39m10000\u001b[39m, \u001b[39m#gpus=1\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/openSource/ts_inductive/orbital_ode.ipynb#X16sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m                             \u001b[39m#gradient_clip_val=0.5,\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/openSource/ts_inductive/orbital_ode.ipynb#X16sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m                             callbacks\u001b[39m=\u001b[39m[early_stop_callback],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/openSource/ts_inductive/orbital_ode.ipynb#X16sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m                             logger\u001b[39m=\u001b[39mlogger,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/openSource/ts_inductive/orbital_ode.ipynb#X16sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m                             enable_progress_bar\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/openSource/ts_inductive/orbital_ode.ipynb#X16sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, train_set, valid_set)\n",
      "File \u001b[1;32mc:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:768\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    749\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    750\u001b[0m \u001b[39mRuns the full optimization routine.\u001b[39;00m\n\u001b[0;32m    751\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[39m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[0;32m    766\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    767\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[1;32m--> 768\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    769\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    770\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:721\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[1;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    719\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    720\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 721\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    722\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[0;32m    723\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[1;32mc:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:809\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    805\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[0;32m    806\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_ckpt_path(\n\u001b[0;32m    807\u001b[0m     ckpt_path, model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    808\u001b[0m )\n\u001b[1;32m--> 809\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[0;32m    811\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[0;32m    812\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1234\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1230\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[0;32m   1232\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[1;32m-> 1234\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[0;32m   1236\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1237\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[1;32mc:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1321\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[0;32m   1320\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[1;32m-> 1321\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[1;32mc:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1351\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1349\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[0;32m   1350\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1351\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[1;32mc:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    205\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:269\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher\u001b[39m.\u001b[39msetup(\n\u001b[0;32m    266\u001b[0m     dataloader, batch_to_device\u001b[39m=\u001b[39mpartial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_call_strategy_hook, \u001b[39m\"\u001b[39m\u001b[39mbatch_to_device\u001b[39m\u001b[39m\"\u001b[39m, dataloader_idx\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m    267\u001b[0m )\n\u001b[0;32m    268\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 269\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[1;32mc:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    205\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\training_epoch_loop.py:208\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_started()\n\u001b[0;32m    207\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_batch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 208\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_loop\u001b[39m.\u001b[39;49mrun(batch, batch_idx)\n\u001b[0;32m    210\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[0;32m    212\u001b[0m \u001b[39m# update non-plateau LR schedulers\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[39m# update epoch-interval ones only when we are at the end of training epoch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    205\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\loops\\batch\\training_batch_loop.py:88\u001b[0m, in \u001b[0;36mTrainingBatchLoop.advance\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mautomatic_optimization:\n\u001b[0;32m     87\u001b[0m     optimizers \u001b[39m=\u001b[39m _get_active_optimizers(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moptimizers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moptimizer_frequencies, batch_idx)\n\u001b[1;32m---> 88\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer_loop\u001b[39m.\u001b[39;49mrun(split_batch, optimizers, batch_idx)\n\u001b[0;32m     89\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_loop\u001b[39m.\u001b[39mrun(split_batch, batch_idx)\n",
      "File \u001b[1;32mc:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\loops\\base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    205\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:203\u001b[0m, in \u001b[0;36mOptimizerLoop.advance\u001b[1;34m(self, batch, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madvance\u001b[39m(\u001b[39mself\u001b[39m, batch: Any, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:  \u001b[39m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m--> 203\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_optimization(\n\u001b[0;32m    204\u001b[0m         batch,\n\u001b[0;32m    205\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_idx,\n\u001b[0;32m    206\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizers[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptim_progress\u001b[39m.\u001b[39;49moptimizer_position],\n\u001b[0;32m    207\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer_idx,\n\u001b[0;32m    208\u001b[0m     )\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         \u001b[39m# automatic optimization assumes a loss needs to be returned for extras to be considered as the batch\u001b[39;00m\n\u001b[0;32m    211\u001b[0m         \u001b[39m# would be skipped otherwise\u001b[39;00m\n\u001b[0;32m    212\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer_idx] \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39masdict()\n",
      "File \u001b[1;32mc:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:256\u001b[0m, in \u001b[0;36mOptimizerLoop._run_optimization\u001b[1;34m(self, split_batch, batch_idx, optimizer, opt_idx)\u001b[0m\n\u001b[0;32m    249\u001b[0m         closure()\n\u001b[0;32m    251\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[39m# BACKWARD PASS\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \u001b[39m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 256\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_step(optimizer, opt_idx, batch_idx, closure)\n\u001b[0;32m    258\u001b[0m result \u001b[39m=\u001b[39m closure\u001b[39m.\u001b[39mconsume_result()\n\u001b[0;32m    260\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    261\u001b[0m     \u001b[39m# if no result, user decided to skip optimization\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[39m# otherwise update running loss + reset accumulated loss\u001b[39;00m\n\u001b[0;32m    263\u001b[0m     \u001b[39m# TODO: find proper way to handle updating running loss\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:369\u001b[0m, in \u001b[0;36mOptimizerLoop._optimizer_step\u001b[1;34m(self, optimizer, opt_idx, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_ready()\n\u001b[0;32m    368\u001b[0m \u001b[39m# model hook\u001b[39;00m\n\u001b[1;32m--> 369\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_lightning_module_hook(\n\u001b[0;32m    370\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39moptimizer_step\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    371\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mcurrent_epoch,\n\u001b[0;32m    372\u001b[0m     batch_idx,\n\u001b[0;32m    373\u001b[0m     optimizer,\n\u001b[0;32m    374\u001b[0m     opt_idx,\n\u001b[0;32m    375\u001b[0m     train_step_and_backward_closure,\n\u001b[0;32m    376\u001b[0m     on_tpu\u001b[39m=\u001b[39;49m\u001b[39misinstance\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49maccelerator, TPUAccelerator),\n\u001b[0;32m    377\u001b[0m     using_native_amp\u001b[39m=\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mamp_backend \u001b[39m==\u001b[39;49m AMPType\u001b[39m.\u001b[39;49mNATIVE),\n\u001b[0;32m    378\u001b[0m     using_lbfgs\u001b[39m=\u001b[39;49mis_lbfgs,\n\u001b[0;32m    379\u001b[0m )\n\u001b[0;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m should_accumulate:\n\u001b[0;32m    382\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_completed()\n",
      "File \u001b[1;32mc:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1593\u001b[0m, in \u001b[0;36mTrainer._call_lightning_module_hook\u001b[1;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1590\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[0;32m   1592\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1593\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1595\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m   1596\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\core\\lightning.py:1625\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[1;34m(self, epoch, batch_idx, optimizer, optimizer_idx, optimizer_closure, on_tpu, using_native_amp, using_lbfgs)\u001b[0m\n\u001b[0;32m   1543\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimizer_step\u001b[39m(\n\u001b[0;32m   1544\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1545\u001b[0m     epoch: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1552\u001b[0m     using_lbfgs: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   1553\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1554\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1555\u001b[0m \u001b[39m    Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[0;32m   1556\u001b[0m \u001b[39m    each optimizer.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1623\u001b[0m \n\u001b[0;32m   1624\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1625\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49moptimizer_closure)\n",
      "File \u001b[1;32mc:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:168\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[1;34m(self, closure, **kwargs)\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\u001b[39m\"\u001b[39m\u001b[39mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    167\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy\u001b[39m.\u001b[39moptimizer_step(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_idx, closure, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    170\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_after_step()\n\u001b[0;32m    172\u001b[0m \u001b[39mreturn\u001b[39;00m step_output\n",
      "File \u001b[1;32mc:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:193\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[1;34m(self, optimizer, opt_idx, closure, model, **kwargs)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[39m\"\"\"Performs the actual optimizer step.\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \n\u001b[0;32m    185\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[39m    **kwargs: Any extra arguments to ``optimizer.step``\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    192\u001b[0m model \u001b[39m=\u001b[39m model \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module\n\u001b[1;32m--> 193\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39moptimizer_step(model, optimizer, opt_idx, closure, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision_plugin.py:155\u001b[0m, in \u001b[0;36mPrecisionPlugin.optimizer_step\u001b[1;34m(self, model, optimizer, optimizer_idx, closure, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model, pl\u001b[39m.\u001b[39mLightningModule):\n\u001b[0;32m    154\u001b[0m     closure \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_closure, model, optimizer, optimizer_idx, closure)\n\u001b[1;32m--> 155\u001b[0m \u001b[39mreturn\u001b[39;00m optimizer\u001b[39m.\u001b[39mstep(closure\u001b[39m=\u001b[39mclosure, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\torch\\optim\\optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m     87\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m---> 88\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\torch\\optim\\adam.py:100\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     99\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[1;32m--> 100\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[0;32m    102\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[0;32m    103\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision_plugin.py:140\u001b[0m, in \u001b[0;36mPrecisionPlugin._wrap_closure\u001b[1;34m(self, model, optimizer, optimizer_idx, closure)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrap_closure\u001b[39m(\n\u001b[0;32m    128\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    129\u001b[0m     model: \u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    132\u001b[0m     closure: Callable[[], Any],\n\u001b[0;32m    133\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    134\u001b[0m     \u001b[39m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[39m    ``on_before_optimizer_step`` hook is called.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \n\u001b[0;32m    137\u001b[0m \u001b[39m    The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[39m    consistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m     closure_result \u001b[39m=\u001b[39m closure()\n\u001b[0;32m    141\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_closure(model, optimizer, optimizer_idx)\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m closure_result\n",
      "File \u001b[1;32mc:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:148\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Tensor]:\n\u001b[1;32m--> 148\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclosure(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    149\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\u001b[39m.\u001b[39mloss\n",
      "File \u001b[1;32mc:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:143\u001b[0m, in \u001b[0;36mClosure.closure\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_zero_grad_fn()\n\u001b[0;32m    142\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m step_output\u001b[39m.\u001b[39mclosure_loss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 143\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backward_fn(step_output\u001b[39m.\u001b[39;49mclosure_loss)\n\u001b[0;32m    145\u001b[0m \u001b[39mreturn\u001b[39;00m step_output\n",
      "File \u001b[1;32mc:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\loops\\optimization\\optimizer_loop.py:311\u001b[0m, in \u001b[0;36mOptimizerLoop._make_backward_fn.<locals>.backward_fn\u001b[1;34m(loss)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward_fn\u001b[39m(loss: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 311\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_strategy_hook(\u001b[39m\"\u001b[39;49m\u001b[39mbackward\u001b[39;49m\u001b[39m\"\u001b[39;49m, loss, optimizer, opt_idx)\n\u001b[0;32m    313\u001b[0m     \u001b[39m# check if model weights are nan\u001b[39;00m\n\u001b[0;32m    314\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_terminate_on_nan:\n",
      "File \u001b[1;32mc:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1763\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[1;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1760\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m   1762\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m-> 1763\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1765\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:168\u001b[0m, in \u001b[0;36mStrategy.backward\u001b[1;34m(self, closure_loss, *args, **kwargs)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_backward(closure_loss)\n\u001b[0;32m    166\u001b[0m closure_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mpre_backward(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, closure_loss)\n\u001b[1;32m--> 168\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mbackward(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, closure_loss, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    170\u001b[0m closure_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mpost_backward(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module, closure_loss)\n\u001b[0;32m    171\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_backward(closure_loss)\n",
      "File \u001b[1;32mc:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision_plugin.py:80\u001b[0m, in \u001b[0;36mPrecisionPlugin.backward\u001b[1;34m(self, model, closure_loss, optimizer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[39m# do backward pass\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[39mif\u001b[39;00m model \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(model, pl\u001b[39m.\u001b[39mLightningModule):\n\u001b[1;32m---> 80\u001b[0m     model\u001b[39m.\u001b[39mbackward(closure_loss, optimizer, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     81\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     82\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_backward(closure_loss, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\core\\lightning.py:1370\u001b[0m, in \u001b[0;36mLightningModule.backward\u001b[1;34m(self, loss, optimizer, optimizer_idx, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1353\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward\u001b[39m(\n\u001b[0;32m   1354\u001b[0m     \u001b[39mself\u001b[39m, loss: Tensor, optimizer: Optional[Optimizer], optimizer_idx: Optional[\u001b[39mint\u001b[39m], \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m   1355\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1356\u001b[0m     \u001b[39m\"\"\"Called to perform backward on the loss returned in :meth:`training_step`. Override this hook with your\u001b[39;00m\n\u001b[0;32m   1357\u001b[0m \u001b[39m    own implementation if you need to.\u001b[39;00m\n\u001b[0;32m   1358\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1368\u001b[0m \u001b[39m            loss.backward()\u001b[39;00m\n\u001b[0;32m   1369\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1370\u001b[0m     loss\u001b[39m.\u001b[39mbackward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# not a single epoch in 1 hour\n",
    "\n",
    "\n",
    "\n",
    "for mult in [1]:\n",
    "\n",
    "    train_set = list(zip(X, y))\n",
    "    train_set_size = int(len(train_set) * 0.8)\n",
    "    valid_set_size = len(train_set) - train_set_size\n",
    "    train_set, valid_set = random_split(train_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    best_model = None\n",
    "    best_score = 1e15\n",
    "    times = 3\n",
    "\n",
    "    for _ in range(times):\n",
    "        diff_model = GnnLogLinearModel()#.to(device)\n",
    "        model = ODEModule(diff_model)#.to(device)\n",
    "        y_hat = model.forward(X)\n",
    "        loss = model.loss(y_hat, y)\n",
    "        if loss < best_score:\n",
    "            print(loss)\n",
    "            best_score = loss\n",
    "            best_model = model\n",
    "\n",
    "\n",
    "    model = best_model\n",
    "    model.steps = 3\n",
    "    early_stop_callback = EarlyStopping(monitor=\"validation_loss\", patience=300, verbose=False, mode=\"min\")\n",
    "\n",
    "    train_set = DataLoader(train_set, shuffle=True, batch_size=128)\n",
    "    valid_set = DataLoader(valid_set, shuffle=True, batch_size=1000)\n",
    "\n",
    "    logger = TensorBoardLogger(\"lightning_logs\", name=f'gnn_log_linear_ode_{model.steps}') # _masses, hidden_multiple\n",
    "\n",
    "    # train with both splits\n",
    "    trainer = pl.Trainer(gpus=1, max_epochs=10000, #gpus=1\n",
    "                                #gradient_clip_val=0.5,\n",
    "                                callbacks=[early_stop_callback],\n",
    "                                logger=logger,\n",
    "                                enable_progress_bar=False)\n",
    "\n",
    "    trainer.fit(model, train_set, valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_model = GnnLogLinearModel()#.to(device)\n",
    "model = ODEModule(diff_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_local = []\n",
    "x = X[:128]\n",
    "model.steps = 2\n",
    "x_local.append(x)\n",
    "for i in range(model.steps):\n",
    "    x_local.append(x_local[i] + 0 * model.dt * model.diff_model(x_local[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = x_local[-1]\n",
    "loss = model.loss(y_hat, y[:128])\n",
    "# backward pass takes an eternity (even on a batch of 16)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[nan, nan, nan]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_model = GnnLogLinearModel()#.to(device)\n",
    "model = ODEModule(diff_model)\n",
    "\n",
    "x_local_0 = X[:128]\n",
    "x_local_1 = x_local_0 + 0 * model.dt * model.diff_model(x_local_0) + 1\n",
    "x_local_2 = x_local_1 + 0 * model.dt * model.diff_model(x_local_1) + 1\n",
    "x_local_3 = x_local_2 + 0 * model.dt * model.diff_model(x_local_2) + 1\n",
    "\n",
    "y_hat = x_local_3 # x_local_2??\n",
    "loss = model.loss(y_hat, y[:128])\n",
    "# backward pass takes an eternity (even on a batch of 16)\n",
    "loss.backward()\n",
    "next(model.parameters()).grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000],\n",
       "         [-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000],\n",
       "         [-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000],\n",
       "         ...,\n",
       "         [-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000],\n",
       "         [-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000],\n",
       "         [-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000]],\n",
       "\n",
       "        [[-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000],\n",
       "         [-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000],\n",
       "         [-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000],\n",
       "         ...,\n",
       "         [-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000],\n",
       "         [-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000],\n",
       "         [-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000]],\n",
       "\n",
       "        [[-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000],\n",
       "         [-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000],\n",
       "         [-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000],\n",
       "         ...,\n",
       "         [-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000],\n",
       "         [-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000],\n",
       "         [-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000],\n",
       "         [-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000],\n",
       "         [-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000],\n",
       "         ...,\n",
       "         [-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000],\n",
       "         [-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000],\n",
       "         [-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000]],\n",
       "\n",
       "        [[-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000],\n",
       "         [-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000],\n",
       "         [-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000],\n",
       "         ...,\n",
       "         [-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000],\n",
       "         [-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000],\n",
       "         [-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000]],\n",
       "\n",
       "        [[-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000],\n",
       "         [-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000],\n",
       "         [-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000],\n",
       "         ...,\n",
       "         [-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000],\n",
       "         [-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000],\n",
       "         [-3.0000, -3.0000, -3.0000,  ..., -3.0000, -3.0000, -3.0000]]],\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_local_0 - x_local_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[0.2396, 0.0866, 0.2170]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([-0.3955], requires_grad=True)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: lightning_logs\\gnn_log_linear_ode_4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0.025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type              | Params\n",
      "-------------------------------------------------\n",
      "0 | diff_model | GnnLogLinearModel | 4     \n",
      "1 | ode_model  | NeuralODE         | 4     \n",
      "-------------------------------------------------\n",
      "4         Trainable params\n",
      "0         Non-trainable params\n",
      "4         Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    }
   ],
   "source": [
    "for mult in [1]:\n",
    "\n",
    "    train_set = list(zip(X, y))\n",
    "    train_set_size = int(len(train_set) * 0.8)\n",
    "    valid_set_size = len(train_set) - train_set_size\n",
    "    train_set, valid_set = random_split(train_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    model.steps += 1\n",
    "    model.dt = dt * steps / model.steps\n",
    "    print(model.steps, model.dt)\n",
    "\n",
    "    early_stop_callback = EarlyStopping(monitor=\"validation_loss\", patience=300, verbose=False, mode=\"min\")\n",
    "\n",
    "    train_set = DataLoader(train_set, shuffle=True, batch_size=128)\n",
    "    valid_set = DataLoader(valid_set, shuffle=True, batch_size=1000)\n",
    "\n",
    "    logger = TensorBoardLogger(\"lightning_logs\", name=f'gnn_log_linear_ode_{model.steps}') # _masses, hidden_multiple\n",
    "\n",
    "    # train with both splits\n",
    "    trainer = pl.Trainer(gpus=1, max_epochs=10000, #gpus=1\n",
    "                                #gradient_clip_val=0.5,\n",
    "                                callbacks=[early_stop_callback],\n",
    "                                logger=logger,\n",
    "                                enable_progress_bar=False)\n",
    "\n",
    "    trainer.fit(model, train_set, valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type              | Params\n",
      "-------------------------------------------------\n",
      "0 | diff_model | GnnLogLinearModel | 4     \n",
      "1 | ode_model  | NeuralODE         | 4     \n",
      "-------------------------------------------------\n",
      "4         Trainable params\n",
      "0         Non-trainable params\n",
      "4         Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    }
   ],
   "source": [
    "for mult in [1]:\n",
    "\n",
    "    train_set = list(zip(X, y))\n",
    "    train_set_size = int(len(train_set) * 0.8)\n",
    "    valid_set_size = len(train_set) - train_set_size\n",
    "    train_set, valid_set = random_split(train_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "    model.steps = 1\n",
    "    model.dt = dt * steps / model.steps\n",
    "    print(model.steps, model.dt)\n",
    "\n",
    "    early_stop_callback = EarlyStopping(monitor=\"validation_loss\", patience=300, verbose=False, mode=\"min\")\n",
    "\n",
    "    train_set = DataLoader(train_set, shuffle=True, batch_size=128)\n",
    "    valid_set = DataLoader(valid_set, shuffle=True, batch_size=1000)\n",
    "\n",
    "    logger = TensorBoardLogger(\"lightning_logs\", name=f'gnn_log_linear_ode_{model.steps}') # _masses, hidden_multiple\n",
    "\n",
    "    # train with both splits\n",
    "    trainer = pl.Trainer(gpus=1, max_epochs=10000, #gpus=1\n",
    "                                #gradient_clip_val=0.5,\n",
    "                                callbacks=[early_stop_callback],\n",
    "                                logger=logger,\n",
    "                                enable_progress_bar=False)\n",
    "\n",
    "    trainer.fit(model, train_set, valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[nan, nan, nan]], requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.diff_model.formula.weight"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type              | Params\n",
      "-------------------------------------------------\n",
      "0 | diff_model | GnnLogLinearModel | 4     \n",
      "1 | ode_model  | NeuralODE         | 4     \n",
      "-------------------------------------------------\n",
      "4         Trainable params\n",
      "0         Non-trainable params\n",
      "4         Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.07479238510131836,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Sanity Checking",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2d9ae1e07f141c8a91635ebec1af9f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:486: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.028936386108398438,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Training",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b76f0df68c44a4a1ee2a8b826d38a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02892446517944336,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "265a6c81dd4649559f4e5be5d370bfd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.07379746437072754,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Validation",
       "rate": null,
       "total": null,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2458a1e61386487f893a2c63db35895f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:724: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "early_stop_callback = EarlyStopping(monitor=\"validation_loss\", patience=300, verbose=False, mode=\"min\")\n",
    "\n",
    "train_set = list(zip(X, y))\n",
    "train_set = list(zip(X, y))\n",
    "train_set_size = int(len(train_set) * 0.8)\n",
    "valid_set_size = len(train_set) - train_set_size\n",
    "train_set, valid_set = random_split(train_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_set = DataLoader(train_set, shuffle=True, batch_size=128)\n",
    "valid_set = DataLoader(valid_set, shuffle=True, batch_size=1000)\n",
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name='gnn_log_linear_mult1') # _masses, hidden_multiple\n",
    "\n",
    "# train with both splits\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=10000,\n",
    "                            #gradient_clip_val=0.5,\n",
    "                            callbacks=[early_stop_callback],\n",
    "                            logger=logger)\n",
    "\n",
    "trainer.fit(model, train_set, valid_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[40.4088],\n",
       "         [ 2.9725],\n",
       "         [ 5.6881],\n",
       "         [ 8.0561],\n",
       "         [32.0705],\n",
       "         [ 2.0293],\n",
       "         [ 1.0138],\n",
       "         [59.0399],\n",
       "         [60.7772],\n",
       "         [ 3.3891]]),\n",
       " tensor([[[39.0646],\n",
       "          [ 2.9494],\n",
       "          [ 5.6877],\n",
       "          [ 8.0410],\n",
       "          [31.9989],\n",
       "          [ 2.0037],\n",
       "          [ 0.9688],\n",
       "          [58.7323],\n",
       "          [60.7730],\n",
       "          [ 3.3401]]], device='cuda:0', grad_fn=<PowBackward1>))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare fitted masses to ground truth\n",
    "g * m[0],  model.masses ** model.formula.weight[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to run tensorboard in the terminal:\n",
    "# tensorboard --logdir lightning_logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('torch_pl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4741426e26a5e6a637207f4863e4b645de3b3c5f81c70cde841ac5e1e8af37aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
