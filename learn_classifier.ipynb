{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from bounce import *\n",
    "from torch.utils.data import DataLoader\n",
    "from pysr import PySRRegressor\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import pickle\n",
    "\n",
    "loader = DataLoader(\n",
    "    BouncyBallsDataBounceRatioLabels(0.5),\n",
    "    batch_size=4096,\n",
    "    drop_last=True,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "x, y, l = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 37.0000, 332.5000,   0.0000,  ...,  11.0000,   0.8754,   0.5194],\n",
       "         [ 37.0000, 394.2500,   0.0000,  ...,  11.0000,   0.8754,   0.5194],\n",
       "         [ 42.4422, 392.2435, 160.5849,  ...,  11.0000,   0.8754,   0.5194],\n",
       "         ...,\n",
       "         [ 67.7732, 479.5254,  38.8028,  ...,  22.0000,   0.8761,   0.9750],\n",
       "         [ 68.4199, 480.5545,  38.8028,  ...,  22.0000,   0.8761,   0.9750],\n",
       "         [ 69.0666, 481.8336,  38.8028,  ...,  22.0000,   0.8761,   0.9750]]),\n",
       " tensor([[  37.0000,  333.5000,    0.0000,   75.0000],\n",
       "         [  37.0000,  400.0000,  160.5849, -232.3085],\n",
       "         [  45.1186,  388.8717,  160.5849, -187.3085],\n",
       "         ...,\n",
       "         [  68.4199,  480.5545,   38.8028,   76.7482],\n",
       "         [  69.0666,  481.8336,   38.8028,   91.7482],\n",
       "         [  69.7133,  483.3628,   38.8028,  106.7482]]),\n",
       " tensor([[0],\n",
       "         [1],\n",
       "         [0],\n",
       "         ...,\n",
       "         [0],\n",
       "         [0],\n",
       "         [0]], dtype=torch.int32))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('base_model.pk', 'rb') as sr_model_file:\n",
    "    sr_model = pickle.load(sr_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model for predicting category (bounce or not)\n",
    "\n",
    "class BaseModule(pl.LightningModule):\n",
    "    def __init__(self, only_position=False):\n",
    "        super(BaseModule, self).__init__()\n",
    "        self.only_position = only_position\n",
    "        self.input_size = 8\n",
    "        self.output_size = 1\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        #error = (y - sr_model.predict(x)).abs().mean(dim=1)\n",
    "        \n",
    "        error = (y - torch.Tensor(sr_model.predict(x.cpu())).cuda()).abs().mean(dim=1, keepdims=True)\n",
    "        category = (error > error.mean(0, keepdims=True)).type(torch.float)\n",
    "        y_hat = torch.sigmoid(self.forward(x))\n",
    "        # could think of scaling too.\n",
    "        #mae, rmse, mse\n",
    "        loss = F.mse_loss(y_hat, category)\n",
    "        self.log('train_loss', loss.item(), on_epoch=True, on_step=False)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-5)\n",
    "        return optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: soft_min of free fall and MLP\n",
    "\n",
    "class BaseModule(pl.LightningModule):\n",
    "    def __init__(self, only_position=False):\n",
    "        super(BaseModule, self).__init__()\n",
    "        self.only_position = only_position\n",
    "        self.input_size = 8\n",
    "        self.output_size = 4\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        y_sr = torch.Tensor(sr_model.predict(x.cpu())).cuda()\n",
    "        y_mlp = self.forward(x)\n",
    "\n",
    "        loss_sr = F.l1_loss(y_sr, y, reduction='none').mean(axis=1)\n",
    "        loss_mlp = F.l1_loss(y_mlp, y, reduction='none').mean(axis=1)\n",
    "        # without .mean, this would be a dimensionswise soft_min\n",
    "\n",
    "        # compared to mean error (otherwise this caused the softmin to give 0+eps for most bigger values)\n",
    "        # think about this: could also compare to the mean of the two, this way it says we should weight something up if it's good relative to other predictions of given branch\n",
    "        loss_sr_ = loss_sr / (loss_sr.mean() + 1e-6)\n",
    "        loss_mlp_ = loss_mlp / (loss_mlp.mean() + 1e-6)\n",
    "\n",
    "        # add the epsilon to the weights - otherwise if both losses very big, none of them will try to learn the data points. \n",
    "        # some better idea?\n",
    "        c_sr = torch.exp(-loss_sr_) + 1e-6\n",
    "        c_mlp = torch.exp(-loss_mlp_) + 1e-6\n",
    "        c_sum = c_sr + c_mlp\n",
    "        c_sr /= c_sum\n",
    "        c_mlp /= c_sum\n",
    "\n",
    "        loss = c_sr * loss_sr + c_mlp * loss_mlp\n",
    "        loss = loss.mean()\n",
    "\n",
    "        self.log('sr_loss', loss_sr.mean().item(), on_epoch=True, on_step=False)\n",
    "        self.log('mlp_loss', loss_mlp.mean().item(), on_epoch=True, on_step=False)\n",
    "        self.log('c_sr_loss', (c_sr * loss_sr).mean().item(), on_epoch=True, on_step=False)\n",
    "        self.log('c_mlp_loss', (c_mlp * loss_mlp).mean().item(), on_epoch=True, on_step=False)\n",
    "        self.log('train_loss', loss.item(), on_epoch=True, on_step=False)\n",
    "        self.log('error_corr', torch.corrcoef(torch.stack([loss_sr, loss_mlp]))[0,1].item(), on_epoch=True, on_step=False)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-5)\n",
    "        return optimizer \n",
    "\n",
    "class MLP_model(BaseModule):\n",
    "    def __init__(self, hidden_sizes, only_position=False):\n",
    "        super(MLP_model, self).__init__(only_position)\n",
    "        self.model = MLP(self.input_size, hidden_sizes, self.output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModule(pl.LightningModule):\n",
    "    def __init__(self, only_position=False):\n",
    "        super(BaseModule, self).__init__()\n",
    "        self.only_position = only_position\n",
    "        self.input_size = 8\n",
    "        self.output_size = 4\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        y_mlp1 = self.mlp1(x)\n",
    "        y_mlp2 = self.mlp2(x)\n",
    "\n",
    "        # should we use L1 or L2 loss? \n",
    "        loss_mlp1 = F.l1_loss(y_mlp1, y, reduction='none').mean(axis=1)\n",
    "        loss_mlp2 = F.l1_loss(y_mlp2, y, reduction='none').mean(axis=1)\n",
    "        \n",
    "        # without .mean, this would be a dimensionswise soft_min\n",
    "\n",
    "        # add the epsilon to the weights - otherwise if both losses very big, none of them will try to learn the data points. \n",
    "        # some better idea?\n",
    "        c_mlp1 = torch.exp(-loss_mlp1) + 1e-6\n",
    "        c_mlp2 = torch.exp(-loss_mlp2) + 1e-6\n",
    "        c_sum = c_mlp1 + c_mlp2\n",
    "        c_mlp1 /= c_sum\n",
    "        c_mlp2 /= c_sum\n",
    "\n",
    "        loss = c_mlp1 * loss_mlp1 + c_mlp2 * loss_mlp2\n",
    "        loss = loss.mean()\n",
    "\n",
    "        self.log('mlp1_loss', loss_mlp1.mean().item(), on_epoch=True, on_step=False)\n",
    "        self.log('mlp2_loss', loss_mlp2.mean().item(), on_epoch=True, on_step=False)\n",
    "        self.log('c_mlp1_loss', (c_mlp1 * loss_mlp1).mean().item(), on_epoch=True, on_step=False)\n",
    "        self.log('c_mlp2_loss', (c_mlp2 * loss_mlp2).mean().item(), on_epoch=True, on_step=False)\n",
    "        self.log('train_loss', loss.item(), on_epoch=True, on_step=False)\n",
    "        self.log('error_corr', torch.corrcoef(torch.stack([loss_mlp1, loss_mlp2]))[0,1].item(), on_epoch=True, on_step=False)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-5)\n",
    "        return optimizer \n",
    "\n",
    "class MLP_model(BaseModule):\n",
    "    def __init__(self, hidden_sizes1, hidden_sizes2, only_position=False):\n",
    "        super(MLP_model, self).__init__(only_position)\n",
    "        self.mlp1 = MLP(self.input_size, hidden_sizes1, self.output_size)\n",
    "        self.mlp2 = MLP(self.input_size, hidden_sizes2, self.output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mlp1(x), self.mlp2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: no softmin, just MLP\n",
    "\n",
    "\n",
    "class BaseModule(pl.LightningModule):\n",
    "    def __init__(self, only_position=False):\n",
    "        super(BaseModule, self).__init__()\n",
    "        self.only_position = only_position\n",
    "        self.input_size = 8\n",
    "        self.output_size = 4\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        y_mlp1 = self.mlp1(x)\n",
    "        y_mlp2 = self.mlp2(x)\n",
    "        c_mlp = torch.squeeze(F.sigmoid(self.mlpc(x)))\n",
    "\n",
    "        # add some offset to make sure it does not collapse\n",
    "        #padding_weight = 1e-5\n",
    "        #c_mlp = (c_mlp + 0.5 * padding_weight) / (1 + padding_weight)\n",
    "\n",
    "        # should we use L1 or L2 loss? \n",
    "        loss_mlp1 = F.l1_loss(y_mlp1, y, reduction='none').mean(axis=1)\n",
    "        loss_mlp2 = F.l1_loss(y_mlp2, y, reduction='none').mean(axis=1)\n",
    "        \n",
    "        # without .mean, this would be a dimensionswise soft_min\n",
    "\n",
    "        # add the epsilon to the weights - otherwise if both losses very big, none of them will try to learn the data points. \n",
    "        # some better idea?\n",
    "        #c_mlp1 = torch.exp(-loss_mlp1) + 1e-6\n",
    "        #c_mlp2 = torch.exp(-loss_mlp2) + 1e-6\n",
    "        #c_sum = c_mlp1 + c_mlp2\n",
    "        #c_mlp1 /= c_sum\n",
    "        #c_mlp2 /= c_sum\n",
    "\n",
    "        loss = c_mlp * loss_mlp1 + (1-c_mlp) * loss_mlp2\n",
    "        loss = loss.mean()\n",
    "\n",
    "        self.log('mlp1_loss', loss_mlp1.mean().item(), on_epoch=True, on_step=False)\n",
    "        self.log('mlp2_loss', loss_mlp2.mean().item(), on_epoch=True, on_step=False)\n",
    "        self.log('c_mlp1_loss', (c_mlp * loss_mlp1).mean().item(), on_epoch=True, on_step=False)\n",
    "        self.log('c_mlp2_loss', ((1-c_mlp) * loss_mlp2).mean().item(), on_epoch=True, on_step=False)\n",
    "        self.log('train_loss', loss.item(), on_epoch=True, on_step=False)\n",
    "        self.log('error_corr', torch.corrcoef(torch.stack([loss_mlp1, loss_mlp2]))[0,1].item(), on_epoch=True, on_step=False)\n",
    "        self.log('coeff_corr', torch.corrcoef(torch.stack([loss_mlp1, c_mlp]))[0,1].item(), on_epoch=True, on_step=False)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-5)\n",
    "        return optimizer \n",
    "\n",
    "class MLP_model(BaseModule):\n",
    "    def __init__(self, hidden_sizes1, hidden_sizes2, hidden_sizes3, only_position=False):\n",
    "        super(MLP_model, self).__init__(only_position)\n",
    "        self.mlp1 = MLP(self.input_size, hidden_sizes1, self.output_size)\n",
    "        self.mlp2 = MLP(self.input_size, hidden_sizes2, self.output_size)\n",
    "        self.mlpc = MLP(self.input_size, hidden_sizes3, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mlp1(x), self.mlp2(x), torch.squeeze(F.sigmoid(self.mlpc(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model: softmin helps out\n",
    "\n",
    "\n",
    "class BaseModule(pl.LightningModule):\n",
    "    def __init__(self, only_position=False):\n",
    "        super(BaseModule, self).__init__()\n",
    "        self.only_position = only_position\n",
    "        self.input_size = 8\n",
    "        self.output_size = 4\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        y_mlp1 = self.mlp1(x)\n",
    "        y_mlp2 = self.mlp2(x)\n",
    "        c_mlp = torch.squeeze(F.sigmoid(self.mlpc(x)))\n",
    "\n",
    "        # add some offset to make sure it does not collapse\n",
    "        # we can let the soft_min do the padding\n",
    "        padding_weight = 1e-5\n",
    "        c_mlp = (c_mlp + 0.5 * padding_weight) / (1 + padding_weight)\n",
    "\n",
    "        # should we use L1 or L2 loss? \n",
    "        loss_mlp1 = F.l1_loss(y_mlp1, y, reduction='none').mean(axis=1)\n",
    "        loss_mlp2 = F.l1_loss(y_mlp2, y, reduction='none').mean(axis=1)\n",
    "        # without .mean, this would be a dimensionswise soft_min\n",
    "\n",
    "        # compared to mean error (otherwise this caused the softmin to give 0+eps for most bigger values)\n",
    "        loss_mlp1_ = loss_mlp1 / (loss_mlp1.mean() + 1e-6)\n",
    "        loss_mlp2_ = loss_mlp1 / (loss_mlp2.mean() + 1e-6)\n",
    "        \n",
    "\n",
    "        # add the epsilon to the weights - otherwise if both losses very big, none of them will try to learn the data points. \n",
    "        # some better idea?\n",
    "        c_mlp1 = torch.exp(-loss_mlp1_) + 1e-6\n",
    "        c_mlp2 = torch.exp(-loss_mlp2_) + 1e-6\n",
    "        c_sum = c_mlp1 + c_mlp2\n",
    "        c_mlp1 /= c_sum\n",
    "        c_mlp2 /= c_sum\n",
    "\n",
    "        # equal weight of classifier and softmin\n",
    "        c_loss = c_mlp * loss_mlp1 + (1-c_mlp) * loss_mlp2\n",
    "        m_loss = c_mlp1 * loss_mlp1 + c_mlp2 * loss_mlp2\n",
    "        loss = 0.5 * (c_loss + m_loss)\n",
    "        loss = loss.mean()\n",
    "\n",
    "        # enhanced with error correlation maximization and coefficient correlation minimization\n",
    "        loss = loss * (2 + torch.corrcoef(torch.stack([loss_mlp1, loss_mlp2]))[0,1]) * (2 - torch.corrcoef(torch.stack([c_mlp1, c_mlp]))[0,1])\n",
    "\n",
    "        #loss = loss - torch.corrcoef(torch.stack([c_mlp1, c_mlp]))[0,1] #+ torch.corrcoef(torch.stack([loss_mlp1, loss_mlp2]))[0,1] #\n",
    "\n",
    "        self.log('mlp1_loss', loss_mlp1.mean().item(), on_epoch=True, on_step=False)\n",
    "        self.log('mlp2_loss', loss_mlp2.mean().item(), on_epoch=True, on_step=False)\n",
    "        self.log('c_mlp1_loss', (c_mlp * loss_mlp1).mean().item(), on_epoch=True, on_step=False)\n",
    "        self.log('c_mlp2_loss', ((1-c_mlp) * loss_mlp2).mean().item(), on_epoch=True, on_step=False)\n",
    "        self.log('train_loss', loss.item(), on_epoch=True, on_step=False)\n",
    "        self.log('train_c_loss', c_loss.mean().item(), on_epoch=True, on_step=False)\n",
    "        self.log('train_m_loss', m_loss.mean().item(), on_epoch=True, on_step=False)\n",
    "        self.log('error_corr', torch.corrcoef(torch.stack([loss_mlp1, loss_mlp2]))[0,1].item(), on_epoch=True, on_step=False)\n",
    "        self.log('coeff_corr', torch.corrcoef(torch.stack([c_mlp1, c_mlp]))[0,1].item(), on_epoch=True, on_step=False)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-5)\n",
    "        return optimizer \n",
    "\n",
    "class MLP_model(BaseModule):\n",
    "    def __init__(self, hidden_sizes1, hidden_sizes2, hidden_sizes3, only_position=False):\n",
    "        super(MLP_model, self).__init__(only_position)\n",
    "        self.mlp1 = MLP(self.input_size, hidden_sizes1, self.output_size)\n",
    "        self.mlp2 = MLP(self.input_size, hidden_sizes2, self.output_size)\n",
    "        self.mlpc = MLP(self.input_size, hidden_sizes3, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mlp1(x), self.mlp2(x), torch.squeeze(F.sigmoid(self.mlpc(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(pl.LightningModule):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        \n",
    "        super(MLP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes  = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.linears = []\n",
    "        prev_hidden = self.input_size\n",
    "        for hidden_size in self.hidden_sizes:\n",
    "            self.linears.append(torch.nn.Linear(prev_hidden, hidden_size))\n",
    "            prev_hidden = hidden_size\n",
    "        self.linears.append(torch.nn.Linear(prev_hidden, self.output_size))\n",
    "\n",
    "        self.linears = torch.nn.ModuleList(self.linears)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for linear in self.linears[:-1]:\n",
    "            x = self.relu(linear(x))\n",
    "        x = self.linears[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'sm_sr_mlp': MLP_model([256, 256]),\n",
    "    #'double_model_l1': MLP_model([256, 256]),\n",
    "    #'double_mlp_l1': MLP_model([256, 256], [256, 256]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  2.9901,  -5.7961,  -5.5810,  -5.1119],\n",
       "        [  3.1072,  -6.0536,  -5.1749,  -4.6451],\n",
       "        [  3.0261,  -6.2914,  -4.9262,  -4.1240],\n",
       "        ...,\n",
       "        [-28.1328, -24.8551, -26.2985,  35.6199],\n",
       "        [-28.6839, -25.3942, -26.7826,  35.6591],\n",
       "        [-29.1914, -25.9910, -27.2442,  35.7803]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models['sm_sr_mlp'](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True, used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.gpu:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | MLP  | 69.1 K\n",
      "-------------------------------\n",
      "69.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "69.1 K    Total params\n",
      "0.276     Total estimated model params size (MB)\n",
      "c:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66: : 23it [11:51, 30.92s/it, loss=0.809, v_num=0]\n",
      "Epoch 110: : 3it [01:08, 22.67s/it, loss=51.8, v_num=1]"
     ]
    }
   ],
   "source": [
    "# could add early stopping with patience\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "early_stop_callback = EarlyStopping(monitor=\"train_loss\", patience=3000, verbose=False, mode=\"min\")\n",
    "\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    # this is to handle models without parameters that do not need training\n",
    "    try:\n",
    "        logger = TensorBoardLogger(\"lightning_logs\", name=model_name)\n",
    "        trainer = pl.Trainer(gpus=1, max_epochs=10000,\n",
    "                            gradient_clip_val=0.5,\n",
    "                            callbacks=[early_stop_callback],\n",
    "                            logger=logger)\n",
    "                            #accumulate_grad_batches=4)\n",
    "\n",
    "        trainer.fit(model, loader)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    torch.save(model.state_dict(), f'sr_learn_models/{model_name}.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this was the dimensionwise min, but we want to average over the features\n",
    "m = 'c_m_mlp_corr'\n",
    "torch.save(models[m].state_dict(), f'{m}.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run tensorboard in cmd to see the progress\n",
    "tensorboard --logdir lightning_logs"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4741426e26a5e6a637207f4863e4b645de3b3c5f81c70cde841ac5e1e8af37aa"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('torch_pl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
