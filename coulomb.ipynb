{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One pair of (X,y) looks like:\n",
    "- d = 3 # number of dimensions\n",
    "- N # number of objects\n",
    "- X: N x (2 * d + 2) # positions, velocities, mass, charge\n",
    "- y: N x d # accelerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048 * 16\n",
    "visible = 10\n",
    "hidden = 0\n",
    "N = visible + hidden\n",
    "d = 3\n",
    "\n",
    "scale_exp = 5\n",
    "\n",
    "pos = torch.exp(scale_exp * torch.rand(batch_size, N, d))\n",
    "# make it centered at 0\n",
    "pos -= pos.mean(axis=1, keepdim = True) \n",
    "\n",
    "vel = torch.exp(scale_exp * torch.rand(batch_size, N, d))\n",
    "\n",
    "# assign fixed positions, velocities??? (this shouldn't matter for now) to hidden objects (this only works for one that is put in the center for now)\n",
    "pos[:,:hidden,:] *= 0\n",
    "vel[:,:hidden,:] *= 0\n",
    "\n",
    "m = torch.rand(1, N, 1)\n",
    "# hidden mass:\n",
    "m[0,:hidden,0] = m[0,:hidden,0] * 0 + 1\n",
    "\n",
    "m = torch.exp(scale_exp * m)\n",
    "m = m.expand(batch_size,-1,-1)\n",
    "\n",
    "ch = torch.rand(1, N, 1)\n",
    "ch_sign = torch.randint(0, 2, (1, N, 1)) * 2 - 1\n",
    "# hidden mass:\n",
    "ch[0,:hidden,0] = m[0,:hidden,0] * 0 + 1\n",
    "\n",
    "ch = torch.exp(scale_exp * ch) * ch_sign\n",
    "ch = ch.expand(batch_size,-1,-1)\n",
    "\n",
    "dt = 0.01\n",
    "k = 0.5\n",
    "\n",
    "chs = ch.unsqueeze(2).expand(-1,-1,N,-1)\n",
    "ch1 = chs\n",
    "ch2 = chs.transpose(1,2)\n",
    "\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "for _ in range(1):\n",
    "    xs = pos.unsqueeze(2).expand(-1,-1,N,-1)\n",
    "    x1 = xs\n",
    "    x2 = xs.transpose(1,2)\n",
    "\n",
    "    delta_x = x1 - x2\n",
    "    delta_x_norm = (delta_x ** 2).sum(dim=-1, keepdim=True)**0.5 + 1e-9\n",
    "    forces = k * ch1 * ch2 / delta_x_norm ** 2\n",
    "\n",
    "    # the delta_x_norms were offset by a small number to avoid numeric problems\n",
    "    # this is fine, when multiplying by delta_x, the self-self terms are zeroed out\n",
    "    force_vectors = forces * delta_x / delta_x_norm\n",
    "    a = force_vectors.sum(dim=2) / m\n",
    "\n",
    "    X_list.append(torch.cat((pos, vel, ch, m), dim=-1))\n",
    "    y_list.append(a)\n",
    "\n",
    "    # simple 1 step - could use a more intelligent integrator here.\n",
    "    vel += a * dt\n",
    "    pos += vel * dt\n",
    "\n",
    "X = torch.cat(X_list)\n",
    "y = torch.cat(y_list)\n",
    "\n",
    "# remove hidden objects\n",
    "X = X[:,hidden:,:]\n",
    "y = y[:,hidden:,:]\n",
    "\n",
    "# add some random noise\n",
    "y += 0.1 * torch.randn(y.shape) * y.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 51.6732],\n",
       "         [ 44.2164],\n",
       "         [ -1.4977],\n",
       "         ...,\n",
       "         [ -1.2271],\n",
       "         [-40.4799],\n",
       "         [ -4.9338]],\n",
       "\n",
       "        [[ 51.6732],\n",
       "         [ 44.2164],\n",
       "         [ -1.4977],\n",
       "         ...,\n",
       "         [ -1.2271],\n",
       "         [-40.4799],\n",
       "         [ -4.9338]],\n",
       "\n",
       "        [[ 51.6732],\n",
       "         [ 44.2164],\n",
       "         [ -1.4977],\n",
       "         ...,\n",
       "         [ -1.2271],\n",
       "         [-40.4799],\n",
       "         [ -4.9338]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 51.6732],\n",
       "         [ 44.2164],\n",
       "         [ -1.4977],\n",
       "         ...,\n",
       "         [ -1.2271],\n",
       "         [-40.4799],\n",
       "         [ -4.9338]],\n",
       "\n",
       "        [[ 51.6732],\n",
       "         [ 44.2164],\n",
       "         [ -1.4977],\n",
       "         ...,\n",
       "         [ -1.2271],\n",
       "         [-40.4799],\n",
       "         [ -4.9338]],\n",
       "\n",
       "        [[ 51.6732],\n",
       "         [ 44.2164],\n",
       "         [ -1.4977],\n",
       "         ...,\n",
       "         [ -1.2271],\n",
       "         [-40.4799],\n",
       "         [ -4.9338]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModule(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(BaseModule, self).__init__()\n",
    "        self.input_size = 3 # r, ch1, ch2\n",
    "        self.output_size = 1\n",
    "        self.loss = F.mse_loss # torch.log(F.mrse_loss) + angle loss\n",
    "        self.lr = 1e-3\n",
    "        self.wd = 0 #1e-5\n",
    "        # relative mean weighted error - this wasn't helpful at all\n",
    "        # self.loss = lambda y_hat, y: ((y_hat - y).abs() / (y.abs() + 1e-8)).mean()\n",
    "        \n",
    "        self.my_loggers = {\n",
    "            'r_exp': lambda s: s.formula.weight[0][0].item(),\n",
    "            'ch1_exp': lambda s: s.formula.weight[0][1].item(),\n",
    "            'ch2_exp': lambda s: s.formula.weight[0][2].item()\n",
    "        }\n",
    "        \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_hat = self.forward(X)\n",
    "\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.log('train_loss', loss.item(), on_epoch=True, on_step=False)\n",
    "\n",
    "        # log learning terms\n",
    "        for name, fx in self.my_loggers.items():\n",
    "            self.log(name, fx(self), on_epoch=True, on_step=False)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_hat = self.forward(X)\n",
    "\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.log('validation_loss', loss.item(), on_epoch=True, on_step=False)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.wd)\n",
    "        return optimizer \n",
    "\n",
    "class GnnLogLinearModel(BaseModule):\n",
    "    def __init__(self):\n",
    "        super(GnnLogLinearModel, self).__init__()\n",
    "        self.formula = torch.nn.Linear(self.input_size, self.output_size) \n",
    "\n",
    "    def forward(self, X):\n",
    "        N = X.shape[1]\n",
    "        xs = X[:,:,:d].unsqueeze(2).expand(-1,-1,N,-1)\n",
    "        chs = X[:,:,-2:-1].unsqueeze(2).expand(-1,-1,N,-1)\n",
    "        m = X[:,:,-1:]\n",
    "\n",
    "        x1 = xs\n",
    "        x2 = xs.transpose(1,2)\n",
    "\n",
    "        delta_x = x1 - x2\n",
    "        delta_x_norm = (delta_x ** 2).sum(dim=-1, keepdim=True)**0.5 + 1e-9\n",
    "\n",
    "        ch1 = chs\n",
    "        ch2 = chs.transpose(1,2)\n",
    "\n",
    "        inp = torch.cat((delta_x_norm, ch1.abs(), ch2.abs()), dim=-1)\n",
    "\n",
    "        inp_log = torch.log(inp) \n",
    "\n",
    "        # one linear layer\n",
    "        forces_log = self.formula(inp_log) \n",
    "\n",
    "        forces = torch.exp(forces_log) * ch1.sign() * ch2.sign()\n",
    "\n",
    "        # the delta_x_norms were offset by a small number to avoid numeric problems\n",
    "        # this is fine, when multiplying by delta_x, the self-self terms are zeroed out\n",
    "        force_vectors = forces * delta_x / delta_x_norm\n",
    "\n",
    "        # later learn this directionality too (the -1)\n",
    "        return force_vectors.sum(dim=2) / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GnnLogLinearChargeMassModel(BaseModule):\n",
    "    def __init__(self, N=10, formula_given=False):\n",
    "        super(GnnLogLinearChargeMassModel, self).__init__()\n",
    "        #self.formula = torch.nn.Linear(self.input_size, self.output_size) \n",
    "        self.formula = torch.nn.Linear(self.input_size, self.output_size, bias=False) \n",
    "        if formula_given:\n",
    "            #self.formula.weight.requires_grad_(False)\n",
    "            self.formula.weight = torch.nn.Parameter(torch.tensor([[-2.0, 1.0, 1.0]]), requires_grad=False)\n",
    "\n",
    "        # one could be problematic when taking the log, won't set the scale\n",
    "        #self.fixed_mass = torch.nn.Parameter(torch.tensor([[[10.0]]]), requires_grad=False)\n",
    "        #self.other_masses = torch.nn.Parameter(torch.rand(1, N-1, 1), requires_grad=True)\n",
    "        #self.masses = torch.cat((self.fixed_mass, self.other_masses), dim=1).cuda()\n",
    "        self.masses = torch.nn.Parameter(torch.rand(1, N, 1))\n",
    "        self.charges = torch.nn.Parameter(torch.rand(1, N, 1) * 2 - 1) \n",
    "        #self.masses[0,0,0] = 1.0\n",
    "\n",
    "    def forward(self, X):\n",
    "        N = X.shape[1]\n",
    "        batch_size = X.shape[0]\n",
    "        xs = X[:,:,:d].unsqueeze(2).expand(-1,-1,N,-1)\n",
    "        m = self.masses.expand(batch_size,-1,-1)\n",
    "        chs = self.charges.expand(batch_size,-1,-1).unsqueeze(2).expand(-1,-1,N,-1)\n",
    "        x1 = xs\n",
    "        x2 = xs.transpose(1,2)\n",
    "\n",
    "\n",
    "        delta_x = x1 - x2\n",
    "        delta_x_norm = (delta_x ** 2).sum(dim=-1, keepdim=True)**0.5 + 1e-9\n",
    "\n",
    "        ch1 = chs\n",
    "        ch2 = chs.transpose(1,2)\n",
    "\n",
    "        # also avoiding the use of .abs by squaring (as the formula can be learnt either way)\n",
    "        inp = torch.cat((delta_x_norm, ch1 ** 2, ch2 ** 2), dim=-1)\n",
    "\n",
    "        inp_log = torch.log(inp) \n",
    "\n",
    "        # one linear layer\n",
    "        forces_log = self.formula(inp_log) \n",
    "\n",
    "        # this way we avoid using the sign and \n",
    "        forces = ch1 * ch2 * torch.exp(forces_log)\n",
    "\n",
    "        # the delta_x_norms were offset by a small number to avoid numeric problems\n",
    "        # this is fine, when multiplying by delta_x, the self-self terms are zeroed out\n",
    "        force_vectors = forces * delta_x / delta_x_norm\n",
    "\n",
    "        # later learn this directionality too (the -1)\n",
    "        return force_vectors.sum(dim=2) / m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(33055.9883, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7744, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: lightning_logs\\coulomb_log_linear_charge_mass\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type   | Params\n",
      "-----------------------------------\n",
      "0 | formula | Linear | 3     \n",
      "-----------------------------------\n",
      "23        Trainable params\n",
      "0         Non-trainable params\n",
      "23        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "c:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "for mult in [1]: # [1,3,10,20]:\n",
    "\n",
    "    train_set = list(zip(X, y))\n",
    "    train_set_size = int(len(train_set) * 0.8)\n",
    "    valid_set_size = len(train_set) - train_set_size\n",
    "    train_set, valid_set = random_split(train_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "\n",
    "    best_model = None\n",
    "    best_score = 1e15\n",
    "    times = 5\n",
    "\n",
    "    for _ in range(times):\n",
    "        #model = GnnLogLinearModelMult(mult=mult)\n",
    "        #model = GnnLogLinearModel()\n",
    "        model = GnnLogLinearChargeMassModel(10)\n",
    "        #model = GnnLogLinearMassModel(10)\n",
    "        #model = GnnLogLinearHiddenMassModel(10, e=10, formula_given=False)\n",
    "        y_hat = model.forward(X)\n",
    "        loss = model.loss(y_hat, y)\n",
    "        if loss < best_score:\n",
    "            print(loss)\n",
    "            best_score = loss\n",
    "            best_model = model\n",
    "\n",
    "\n",
    "    model = best_model\n",
    "    early_stop_callback = EarlyStopping(monitor=\"validation_loss\", patience=30, verbose=False, mode=\"min\")\n",
    "\n",
    "    train_set = DataLoader(train_set, shuffle=True, batch_size=128)\n",
    "    valid_set = DataLoader(valid_set, shuffle=True, batch_size=1000)\n",
    "\n",
    "    logger = TensorBoardLogger(\"lightning_logs\", name=f'coulomb_log_linear_charge_mass') # _masses, hidden_multiple\n",
    "\n",
    "    # train with both splits\n",
    "    trainer = pl.Trainer(gpus=1, max_epochs=10000,\n",
    "                                #gradient_clip_val=0.5,\n",
    "                                callbacks=[early_stop_callback],\n",
    "                                logger=logger,\n",
    "                                enable_progress_bar=False)\n",
    "\n",
    "    trainer.fit(model, train_set, valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "early_stop_callback = EarlyStopping(monitor=\"validation_loss\", patience=300, verbose=False, mode=\"min\")\n",
    "\n",
    "train_set = DataLoader(train_set, shuffle=True, batch_size=128)\n",
    "valid_set = DataLoader(valid_set, shuffle=True, batch_size=1000)\n",
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name='gnn_log_linear_mult1') # _masses, hidden_multiple\n",
    "\n",
    "# train with both splits\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=10000,\n",
    "                            #gradient_clip_val=0.5,\n",
    "                            callbacks=[early_stop_callback],\n",
    "                            logger=logger)\n",
    "\n",
    "trainer.fit(model, train_set, valid_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 25.8366],\n",
       "         [ 22.1082],\n",
       "         [ -0.7488],\n",
       "         [ 21.0288],\n",
       "         [ -0.7313],\n",
       "         [  3.0027],\n",
       "         [ 10.9575],\n",
       "         [ -0.6136],\n",
       "         [-20.2400],\n",
       "         [ -2.4669]]),\n",
       " Parameter containing:\n",
       " tensor([[[-1.2047],\n",
       "          [-1.1463],\n",
       "          [ 0.1734],\n",
       "          [-1.0718],\n",
       "          [ 0.1755],\n",
       "          [-0.3780],\n",
       "          [-0.7624],\n",
       "          [ 0.1659],\n",
       "          [ 1.0736],\n",
       "          [ 0.3332]]], requires_grad=True))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# it's almost impossible to disentangle these now, how should we now if it is close? (what if we ensure symmtery - exponent should be the same for the two charges?)\n",
    "k * ch[0], model.charges ** 2 ** model.formula.weight[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 62.9773],\n",
       "         [ 74.8648],\n",
       "         [ 88.6603],\n",
       "         [ 14.6456],\n",
       "         [ 60.0125],\n",
       "         [ 10.1003],\n",
       "         [ 48.7612],\n",
       "         [  1.9424],\n",
       "         [101.5216],\n",
       "         [ 61.0661]]),\n",
       " Parameter containing:\n",
       " tensor([[[0.0697],\n",
       "          [0.0886],\n",
       "          [2.5736],\n",
       "          [0.0202],\n",
       "          [2.2849],\n",
       "          [0.1049],\n",
       "          [0.1239],\n",
       "          [0.0823],\n",
       "          [0.1410],\n",
       "          [0.6899]]], requires_grad=True))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[0], model.masses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[40.4088],\n",
       "         [ 2.9725],\n",
       "         [ 5.6881],\n",
       "         [ 8.0561],\n",
       "         [32.0705],\n",
       "         [ 2.0293],\n",
       "         [ 1.0138],\n",
       "         [59.0399],\n",
       "         [60.7772],\n",
       "         [ 3.3891]]),\n",
       " tensor([[[39.0646],\n",
       "          [ 2.9494],\n",
       "          [ 5.6877],\n",
       "          [ 8.0410],\n",
       "          [31.9989],\n",
       "          [ 2.0037],\n",
       "          [ 0.9688],\n",
       "          [58.7323],\n",
       "          [60.7730],\n",
       "          [ 3.3401]]], device='cuda:0', grad_fn=<PowBackward1>))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare fitted masses to ground truth\n",
    "g * m[0],  model.masses ** model.formula.weight[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to run tensorboard in the terminal:\n",
    "# tensorboard --logdir lightning_logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('torch_pl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4741426e26a5e6a637207f4863e4b645de3b3c5f81c70cde841ac5e1e8af37aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
