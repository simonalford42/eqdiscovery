{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from bounce import *\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from utils import pretty_print, functions\n",
    "from utils.symbolic_network import SymbolicNet\n",
    "from utils.regularization import L12Smooth\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "loader = DataLoader(\n",
    "    BouncyBallsDataBounceRatio(0.05),\n",
    "    #BouncyBallsData(),\n",
    "    batch_size=4096,\n",
    "    drop_last=True,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "x, y = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRAIN = 256       # Size of training dataset\n",
    "N_VAL = 100         # Size of validation dataset\n",
    "DOMAIN = (-1, 1)    # Domain of dataset - range from which we sample x\n",
    "# DOMAIN = np.array([[0, -1, -1], [1, 1, 1]])   # Use this format if each input variable has a different domain\n",
    "N_TEST = 100        # Size of test dataset\n",
    "DOMAIN_TEST = (-2, 2)   # Domain of test dataset - should be larger than training domain to test extrapolation\n",
    "NOISE_SD = 0        # Standard deviation of noise for training dataset\n",
    "var_names = [\"x\", \"y\", \"z\"]\n",
    "\n",
    "# Standard deviation of random distribution for weight initializations.\n",
    "init_sd_first = 0.1\n",
    "init_sd_last = 1.0\n",
    "init_sd_middle = 0.5\n",
    "# init_sd_first = 0.5\n",
    "# init_sd_last = 0.5\n",
    "# init_sd_middle = 0.5\n",
    "# init_sd_first = 0.1\n",
    "# init_sd_last = 0.1\n",
    "# init_sd_middle = 0.1\n",
    "\n",
    "\n",
    "def generate_data(func, N, range_min=DOMAIN[0], range_max=DOMAIN[1]):\n",
    "    \"\"\"Generates datasets.\"\"\"\n",
    "    x_dim = len(signature(func).parameters)     # Number of inputs to the function, or, dimensionality of x\n",
    "    x = (range_max - range_min) * torch.rand([N, x_dim]) + range_min\n",
    "    y = torch.tensor([[func(*x_i)] for x_i in x])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "class Benchmark:\n",
    "    \"\"\"Benchmark object just holds the results directory (results_dir) to save to and the hyper-parameters. So it is\n",
    "    assumed all the results in results_dir share the same hyper-parameters. This is useful for benchmarking multiple\n",
    "    functions with the same hyper-parameters.\"\"\"\n",
    "    def __init__(self, results_dir, n_layers=2, reg_weight=5e-3, learning_rate=1e-2,\n",
    "                 n_epochs1=10001, n_epochs2=10001):\n",
    "        \"\"\"Set hyper-parameters\"\"\"\n",
    "        self.activation_funcs = [\n",
    "            *[functions.Constant()] * 2,\n",
    "            *[functions.Identity()] * 4,\n",
    "            *[functions.Square()] * 4,\n",
    "            *[functions.Sin()] * 2,\n",
    "            *[functions.Exp()] * 2,\n",
    "            *[functions.Sigmoid()] * 2,\n",
    "            *[functions.Product()] * 2\n",
    "        ]\n",
    "\n",
    "        self.n_layers = n_layers                # Number of hidden layers\n",
    "        self.reg_weight = reg_weight            # Regularization weight\n",
    "        self.learning_rate = learning_rate\n",
    "        self.summary_step = 1000                # Number of iterations at which to print to screen\n",
    "        self.n_epochs1 = n_epochs1\n",
    "        self.n_epochs2 = n_epochs2\n",
    "\n",
    "        if not os.path.exists(results_dir):\n",
    "            os.makedirs(results_dir)\n",
    "        self.results_dir = results_dir\n",
    "\n",
    "        # Save hyperparameters to file\n",
    "        result = {\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"summary_step\": self.summary_step,\n",
    "            \"n_epochs1\": self.n_epochs1,\n",
    "            \"n_epochs2\": self.n_epochs2,\n",
    "            \"activation_funcs_name\": [func.name for func in self.activation_funcs],\n",
    "            \"n_layers\": self.n_layers,\n",
    "            \"reg_weight\": self.reg_weight,\n",
    "        }\n",
    "        with open(os.path.join(self.results_dir, 'params.pickle'), \"wb+\") as f:\n",
    "            pickle.dump(result, f)\n",
    "\n",
    "    def benchmark(self, func, func_name, trials):\n",
    "        \"\"\"Benchmark the EQL network on data generated by the given function. Print the results ordered by test error.\n",
    "        Arguments:\n",
    "            func: lambda function to generate dataset\n",
    "            func_name: string that describes the function - this will be the directory name\n",
    "            trials: number of trials to train from scratch. Will save the results for each trial.\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"Starting benchmark for function:\\t%s\" % func_name)\n",
    "        print(\"==============================================\")\n",
    "\n",
    "        # Create a new sub-directory just for the specific function\n",
    "        func_dir = os.path.join(self.results_dir, func_name)\n",
    "        if not os.path.exists(func_dir):\n",
    "            os.makedirs(func_dir)\n",
    "\n",
    "        # Train network!\n",
    "        expr_list, error_test_list = self.train(func, func_name, trials, func_dir)\n",
    "\n",
    "        # Sort the results by test error (increasing) and print them to file\n",
    "        # This allows us to easily count how many times it fit correctly.\n",
    "        error_expr_sorted = sorted(zip(error_test_list, expr_list))     # List of (error, expr)\n",
    "        error_test_sorted = [x for x, _ in error_expr_sorted]   # Separating out the errors\n",
    "        expr_list_sorted = [x for _, x in error_expr_sorted]    # Separating out the expr\n",
    "\n",
    "        fi = open(os.path.join(self.results_dir, 'eq_summary.txt'), 'a')\n",
    "        fi.write(\"\\n{}\\n\".format(func_name))\n",
    "        for i in range(trials):\n",
    "            fi.write(\"[%f]\\t\\t%s\\n\" % (error_test_sorted[i], str(expr_list_sorted[i])))\n",
    "        fi.close()\n",
    "\n",
    "    def train(self, func, func_name='', trials=1, func_dir='results/test'):\n",
    "        \"\"\"Train the network to find a given function\"\"\"\n",
    "\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "        print(\"Use cuda:\", use_cuda, \"Device:\", device)\n",
    "\n",
    "        x, y = generate_data(func, N_TRAIN)\n",
    "        data, target = x.to(device), y.to(device)\n",
    "        # x_val, y_val = generate_data(func, N_VAL)\n",
    "        x_test, y_test = generate_data(func, N_TEST, range_min=DOMAIN_TEST[0], range_max=DOMAIN_TEST[1])\n",
    "        test_data, test_target = x_test.to(device), y_test.to(device)\n",
    "\n",
    "        # Setting up the symbolic regression network\n",
    "        x_dim = len(signature(func).parameters)  # Number of input arguments to the function\n",
    "        width = len(self.activation_funcs)\n",
    "        n_double = functions.count_double(self.activation_funcs)\n",
    "\n",
    "        # Arrays to keep track of various quantities as a function of epoch\n",
    "        loss_list = []          # Total loss (MSE + regularization)\n",
    "        error_list = []         # MSE\n",
    "        reg_list = []           # Regularization\n",
    "        error_test_list = []    # Test error\n",
    "\n",
    "        error_test_final = []\n",
    "        eq_list = []\n",
    "\n",
    "        for trial in range(trials):\n",
    "            print(\"Training on function \" + func_name + \" Trial \" + str(trial+1) + \" out of \" + str(trials))\n",
    "\n",
    "            # reinitialize for each trial\n",
    "            net = SymbolicNet(self.n_layers,\n",
    "                              funcs=self.activation_funcs,\n",
    "                              initial_weights=[\n",
    "                                  # kind of a hack for truncated normal\n",
    "                                  torch.fmod(torch.normal(0, init_sd_first, size=(x_dim, width + n_double)), 2),\n",
    "                                  torch.fmod(torch.normal(0, init_sd_middle, size=(width, width + n_double)), 2),\n",
    "                                  torch.fmod(torch.normal(0, init_sd_middle, size=(width, width + n_double)), 2),\n",
    "                                  torch.fmod(torch.normal(0, init_sd_last, size=(width, 1)), 2)\n",
    "                              ]).to(device)\n",
    "\n",
    "            loss_val = np.nan\n",
    "            while np.isnan(loss_val):\n",
    "                # training restarts if gradients blow up\n",
    "                criterion = nn.MSELoss()\n",
    "                optimizer = optim.RMSprop(net.parameters(),\n",
    "                                          lr=self.learning_rate * 10,\n",
    "                                          alpha=0.9,  # smoothing constant\n",
    "                                          eps=1e-10,\n",
    "                                          momentum=0.0,\n",
    "                                          centered=False)\n",
    "\n",
    "                # adaptive learning rate\n",
    "                lmbda = lambda epoch: 0.1\n",
    "                scheduler = optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)\n",
    "                # for param_group in optimizer.param_groups:\n",
    "                #     print(\"Learning rate: %f\" % param_group['lr'])\n",
    "\n",
    "                t0 = time.time()\n",
    "\n",
    "                # First stage of training, preceded by 0th warmup stage\n",
    "                for epoch in range(self.n_epochs1 + 2000):\n",
    "                    optimizer.zero_grad()  # zero the parameter gradients\n",
    "                    outputs = net(data)  # forward pass\n",
    "                    regularization = L12Smooth()\n",
    "                    mse_loss = criterion(outputs, target)\n",
    "\n",
    "                    reg_loss = regularization(net.get_weights_tensor())\n",
    "                    loss = mse_loss + self.reg_weight * reg_loss\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    if epoch % self.summary_step == 0:\n",
    "                        error_val = mse_loss.item()\n",
    "                        reg_val = reg_loss.item()\n",
    "                        loss_val = loss.item()\n",
    "                        error_list.append(error_val)\n",
    "                        reg_list.append(reg_val)\n",
    "                        loss_list.append(loss_val)\n",
    "\n",
    "                        with torch.no_grad():  # test error\n",
    "                            test_outputs = net(test_data)\n",
    "                            test_loss = F.mse_loss(test_outputs, test_target)\n",
    "                            error_test_val = test_loss.item()\n",
    "                            error_test_list.append(error_test_val)\n",
    "\n",
    "                        print(\"Epoch: %d\\tTotal training loss: %f\\tTest error: %f\" % (epoch, loss_val, error_test_val))\n",
    "\n",
    "                        if np.isnan(loss_val) or loss_val > 1000:  # If loss goes to NaN, restart training\n",
    "                            break\n",
    "\n",
    "                    if epoch == 2000:\n",
    "                        scheduler.step()  # lr /= 10\n",
    "\n",
    "                scheduler.step()  # lr /= 10 again\n",
    "\n",
    "                for epoch in range(self.n_epochs2):\n",
    "                    optimizer.zero_grad()  # zero the parameter gradients\n",
    "                    outputs = net(data)\n",
    "                    regularization = L12Smooth()\n",
    "                    mse_loss = criterion(outputs, target)\n",
    "                    reg_loss = regularization(net.get_weights_tensor())\n",
    "                    loss = mse_loss + self.reg_weight * reg_loss\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    if epoch % self.summary_step == 0:\n",
    "                        error_val = mse_loss.item()\n",
    "                        reg_val = reg_loss.item()\n",
    "                        loss_val = loss.item()\n",
    "                        error_list.append(error_val)\n",
    "                        reg_list.append(reg_val)\n",
    "                        loss_list.append(loss_val)\n",
    "\n",
    "                        with torch.no_grad():  # test error\n",
    "                            test_outputs = net(test_data)\n",
    "                            test_loss = F.mse_loss(test_outputs, test_target)\n",
    "                            error_test_val = test_loss.item()\n",
    "                            error_test_list.append(error_test_val)\n",
    "\n",
    "                        print(\"Epoch: %d\\tTotal training loss: %f\\tTest error: %f\" % (epoch, loss_val, error_test_val))\n",
    "\n",
    "                        if np.isnan(loss_val) or loss_val > 1000:  # If loss goes to NaN, restart training\n",
    "                            break\n",
    "\n",
    "                t1 = time.time()\n",
    "\n",
    "            tot_time = t1-t0\n",
    "            print(tot_time)\n",
    "\n",
    "            # Print the expressions\n",
    "            with torch.no_grad():\n",
    "                weights = net.get_weights()\n",
    "                expr = pretty_print.network(weights, self.activation_funcs, var_names[:x_dim])\n",
    "                print(expr)\n",
    "\n",
    "            # Save results\n",
    "            trial_file = os.path.join(func_dir, 'trial%d.pickle' % trial)\n",
    "            results = {\n",
    "                \"weights\": weights,\n",
    "                \"loss_list\": loss_list,\n",
    "                \"error_list\": error_list,\n",
    "                \"reg_list\": reg_list,\n",
    "                \"error_test\": error_test_list,\n",
    "                \"expr\": expr,\n",
    "                \"runtime\": tot_time\n",
    "            }\n",
    "            with open(trial_file, \"wb+\") as f:\n",
    "                pickle.dump(results, f)\n",
    "\n",
    "            error_test_final.append(error_test_list[-1])\n",
    "            eq_list.append(expr)\n",
    "\n",
    "        return eq_list, error_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We use two phases of training, where the first phases uses\n",
    "a learning rate of 10−2\n",
    "and regularization weight λ = 0.05.\n",
    "The second phase uses a learning rate of 10−4\n",
    "and no\n",
    "regularization. The small weights are frozen between the first\n",
    "and second phase with a threshold of α = 0.01. Each phase\n",
    "is trained for 10000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting benchmark for function:\tspeed\n",
      "==============================================\n",
      "Use cuda: True Device: cuda:0\n",
      "Training on function speed Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 269.628571\tTest error: 98177.359375\n",
      "Epoch: 1000\tTotal training loss: 0.710146\tTest error: 0.098058\n",
      "Epoch: 2000\tTotal training loss: 0.893777\tTest error: 23.095833\n",
      "Epoch: 3000\tTotal training loss: 0.319213\tTest error: 0.001261\n",
      "Epoch: 4000\tTotal training loss: 0.317061\tTest error: 0.001940\n",
      "Epoch: 5000\tTotal training loss: 0.319367\tTest error: 0.001573\n",
      "Epoch: 6000\tTotal training loss: 0.316014\tTest error: 0.000301\n",
      "Epoch: 7000\tTotal training loss: 0.316480\tTest error: 0.000579\n",
      "Epoch: 8000\tTotal training loss: 0.316431\tTest error: 0.000621\n",
      "Epoch: 9000\tTotal training loss: 0.317137\tTest error: 0.001095\n",
      "Epoch: 10000\tTotal training loss: 0.316542\tTest error: 0.000487\n",
      "Epoch: 11000\tTotal training loss: 0.318005\tTest error: 0.001293\n",
      "Epoch: 12000\tTotal training loss: 0.316277\tTest error: 0.000589\n",
      "Epoch: 0\tTotal training loss: 0.316336\tTest error: 0.000369\n",
      "Epoch: 1000\tTotal training loss: 0.312906\tTest error: 0.000111\n",
      "Epoch: 2000\tTotal training loss: 0.312899\tTest error: 0.000104\n",
      "Epoch: 3000\tTotal training loss: 0.312898\tTest error: 0.000102\n",
      "Epoch: 4000\tTotal training loss: 0.312898\tTest error: 0.000102\n",
      "Epoch: 5000\tTotal training loss: 0.312898\tTest error: 0.000102\n",
      "Epoch: 6000\tTotal training loss: 0.312898\tTest error: 0.000102\n",
      "Epoch: 7000\tTotal training loss: 0.312898\tTest error: 0.000102\n",
      "Epoch: 8000\tTotal training loss: 0.312898\tTest error: 0.000102\n",
      "Epoch: 9000\tTotal training loss: 0.312898\tTest error: 0.000102\n",
      "Epoch: 10000\tTotal training loss: 0.312898\tTest error: 0.000102\n",
      "416.6205863952637\n",
      "0.997066*x\n"
     ]
    }
   ],
   "source": [
    "from inspect import signature\n",
    "import time\n",
    "bench = Benchmark(results_dir = 'test_results')\n",
    "\n",
    "bench.benchmark(lambda x,y,z: x + 0.016667033*y, func_name=\"speed\", trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting benchmark for function:\tspeed\n",
      "==============================================\n",
      "Use cuda: True Device: cuda:0\n",
      "Training on function speed Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 63.937401\tTest error: 92487.304688\n",
      "Epoch: 1000\tTotal training loss: 0.482008\tTest error: 0.002076\n",
      "Epoch: 2000\tTotal training loss: 0.542434\tTest error: 0.005263\n",
      "Epoch: 3000\tTotal training loss: 0.302932\tTest error: 0.000328\n",
      "Epoch: 4000\tTotal training loss: 0.303226\tTest error: 0.000432\n",
      "Epoch: 5000\tTotal training loss: 0.302859\tTest error: 0.000314\n",
      "Epoch: 6000\tTotal training loss: 0.303039\tTest error: 0.000336\n",
      "Epoch: 7000\tTotal training loss: 0.302851\tTest error: 0.000313\n",
      "Epoch: 8000\tTotal training loss: 0.303774\tTest error: 0.000395\n",
      "Epoch: 9000\tTotal training loss: 0.303601\tTest error: 0.000445\n",
      "Epoch: 10000\tTotal training loss: 0.302887\tTest error: 0.000317\n",
      "Epoch: 11000\tTotal training loss: 0.302873\tTest error: 0.000314\n",
      "Epoch: 12000\tTotal training loss: 0.302854\tTest error: 0.000313\n",
      "Epoch: 0\tTotal training loss: 0.302850\tTest error: 0.000313\n",
      "Epoch: 1000\tTotal training loss: 0.300004\tTest error: 0.000313\n",
      "Epoch: 2000\tTotal training loss: 0.300004\tTest error: 0.000313\n",
      "Epoch: 3000\tTotal training loss: 0.300004\tTest error: 0.000313\n",
      "Epoch: 4000\tTotal training loss: 0.300004\tTest error: 0.000313\n",
      "Epoch: 5000\tTotal training loss: 0.300004\tTest error: 0.000313\n",
      "Epoch: 6000\tTotal training loss: 0.300004\tTest error: 0.000313\n",
      "Epoch: 7000\tTotal training loss: 0.300004\tTest error: 0.000313\n",
      "Epoch: 8000\tTotal training loss: 0.300004\tTest error: 0.000313\n",
      "Epoch: 9000\tTotal training loss: 0.300004\tTest error: 0.000313\n",
      "Epoch: 10000\tTotal training loss: 0.300004\tTest error: 0.000313\n",
      "402.1782383918762\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from inspect import signature\n",
    "import time\n",
    "bench = Benchmark(results_dir = 'test_results')\n",
    "\n",
    "bench.benchmark(lambda x,y,z: 0.016667033*y, func_name=\"speed\", trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting benchmark for function:\tspeed\n",
      "==============================================\n",
      "Use cuda: True Device: cuda:0\n",
      "Training on function speed Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 420.839264\tTest error: 21892716869252546560.000000\n",
      "Epoch: 1000\tTotal training loss: 0.544444\tTest error: 0.140753\n",
      "Epoch: 2000\tTotal training loss: 0.847831\tTest error: 0.141156\n",
      "Epoch: 3000\tTotal training loss: 0.318065\tTest error: 0.001854\n",
      "Epoch: 4000\tTotal training loss: 0.316454\tTest error: 0.000406\n",
      "Epoch: 5000\tTotal training loss: 0.316955\tTest error: 0.001926\n",
      "Epoch: 6000\tTotal training loss: 0.316154\tTest error: 0.000767\n",
      "Epoch: 7000\tTotal training loss: 0.315882\tTest error: 0.000060\n",
      "Epoch: 8000\tTotal training loss: 0.315769\tTest error: 0.000017\n",
      "Epoch: 9000\tTotal training loss: 0.315777\tTest error: 0.000020\n",
      "Epoch: 10000\tTotal training loss: 0.316067\tTest error: 0.000146\n",
      "Epoch: 11000\tTotal training loss: 0.316498\tTest error: 0.001048\n",
      "Epoch: 12000\tTotal training loss: 0.316235\tTest error: 0.000398\n",
      "Epoch: 0\tTotal training loss: 0.316170\tTest error: 0.000246\n",
      "Epoch: 1000\tTotal training loss: 0.312870\tTest error: 0.000043\n",
      "Epoch: 2000\tTotal training loss: 0.312861\tTest error: 0.000044\n",
      "Epoch: 3000\tTotal training loss: 0.312860\tTest error: 0.000044\n",
      "Epoch: 4000\tTotal training loss: 0.312859\tTest error: 0.000044\n",
      "Epoch: 5000\tTotal training loss: 0.312859\tTest error: 0.000044\n",
      "Epoch: 6000\tTotal training loss: 0.312859\tTest error: 0.000044\n",
      "Epoch: 7000\tTotal training loss: 0.312859\tTest error: 0.000044\n",
      "Epoch: 8000\tTotal training loss: 0.312859\tTest error: 0.000044\n",
      "Epoch: 9000\tTotal training loss: 0.312859\tTest error: 0.000044\n",
      "Epoch: 10000\tTotal training loss: 0.312859\tTest error: 0.000044\n",
      "411.67018842697144\n",
      "0.994474*y\n"
     ]
    }
   ],
   "source": [
    "from inspect import signature\n",
    "import time\n",
    "bench = Benchmark(results_dir = 'test_results')\n",
    "\n",
    "bench.benchmark(lambda x,y,z: y, func_name=\"speed\", trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting benchmark for function:\tspeed\n",
      "==============================================\n",
      "Use cuda: True Device: cuda:0\n",
      "Training on function speed Trial 1 out of 1\n",
      "Epoch: 0\tTotal training loss: 9.287493\tTest error: 5521.826172\n",
      "Epoch: 1000\tTotal training loss: 0.814877\tTest error: 0.681931\n",
      "Epoch: 2000\tTotal training loss: 0.645772\tTest error: 0.376138\n",
      "Epoch: 3000\tTotal training loss: 0.328241\tTest error: 0.001390\n",
      "Epoch: 4000\tTotal training loss: 0.321130\tTest error: 0.001399\n",
      "Epoch: 5000\tTotal training loss: 0.314526\tTest error: 0.000514\n",
      "Epoch: 6000\tTotal training loss: 0.316415\tTest error: 0.002403\n",
      "Epoch: 7000\tTotal training loss: 0.315625\tTest error: 0.002531\n",
      "Epoch: 8000\tTotal training loss: 0.315473\tTest error: 0.001007\n",
      "Epoch: 9000\tTotal training loss: 0.315182\tTest error: 0.001077\n",
      "Epoch: 10000\tTotal training loss: 0.315293\tTest error: 0.001676\n",
      "Epoch: 11000\tTotal training loss: 0.315422\tTest error: 0.000728\n",
      "Epoch: 12000\tTotal training loss: 0.315419\tTest error: 0.001798\n",
      "Epoch: 0\tTotal training loss: 0.316061\tTest error: 0.001027\n",
      "Epoch: 1000\tTotal training loss: 0.302475\tTest error: 0.000052\n",
      "Epoch: 2000\tTotal training loss: 0.302426\tTest error: 0.000054\n",
      "Epoch: 3000\tTotal training loss: 0.302407\tTest error: 0.000055\n",
      "Epoch: 4000\tTotal training loss: 0.302398\tTest error: 0.000055\n",
      "Epoch: 5000\tTotal training loss: 0.302393\tTest error: 0.000056\n",
      "Epoch: 6000\tTotal training loss: 0.302391\tTest error: 0.000056\n",
      "Epoch: 7000\tTotal training loss: 0.302390\tTest error: 0.000056\n",
      "Epoch: 8000\tTotal training loss: 0.302389\tTest error: 0.000056\n",
      "Epoch: 9000\tTotal training loss: 0.302389\tTest error: 0.000056\n",
      "Epoch: 10000\tTotal training loss: 0.302389\tTest error: 0.000056\n",
      "407.66676211357117\n",
      "0.995801*x + 0.995702*y\n"
     ]
    }
   ],
   "source": [
    "from inspect import signature\n",
    "import time\n",
    "bench = Benchmark(results_dir = 'test_results')\n",
    "\n",
    "bench.benchmark(lambda x, y: x + y, func_name=\"speed\", trials=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run separately for the different dimensions to get the eqnet result with the current setup\n",
    "# run on real data (or just with some noise initially)\n",
    "# what happens if we run it on the remainder? bounce cases?"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4741426e26a5e6a637207f4863e4b645de3b3c5f81c70cde841ac5e1e8af37aa"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('torch_pl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
