{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One pair of (X,y) looks like:\n",
    "- d = 3 # number of dimensions\n",
    "- N # number of objects\n",
    "- X: N x (2 * d + 1) # positions, velocities, mass\n",
    "- y: N x d # accelerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048 * 16\n",
    "visible = 10\n",
    "hidden = 1\n",
    "N = visible + hidden\n",
    "d = 3\n",
    "\n",
    "scale_exp = 5\n",
    "\n",
    "pos = torch.exp(scale_exp * torch.rand(batch_size, N, d))\n",
    "# make it centered at 0\n",
    "pos -= pos.mean(axis=1, keepdim = True) \n",
    "\n",
    "vel = torch.exp(scale_exp * torch.rand(batch_size, N, d))\n",
    "\n",
    "# assign fixed positions, velocities??? (this shouldn't matter for now) to hidden objects (this only works for one that is put in the center for now)\n",
    "pos[:,:hidden,:] *= 0\n",
    "vel[:,:hidden,:] *= 0\n",
    "\n",
    "m = torch.rand(1, N, 1)\n",
    "# hidden mass:\n",
    "m[0,:hidden,0] = m[0,:hidden,0] * 0 + 1\n",
    "\n",
    "m = torch.exp(scale_exp * m)\n",
    "m = m.expand(batch_size,-1,-1)\n",
    "\n",
    "dt = 0.01\n",
    "g = 0.5\n",
    "\n",
    "ms = m.unsqueeze(2).expand(-1,-1,N,-1)\n",
    "m1 = ms\n",
    "m2 = ms.transpose(1,2)\n",
    "\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "for _ in range(1):\n",
    "    xs = pos.unsqueeze(2).expand(-1,-1,N,-1)\n",
    "    x1 = xs\n",
    "    x2 = xs.transpose(1,2)\n",
    "\n",
    "    delta_x = x1 - x2\n",
    "    delta_x_norm = (delta_x ** 2).sum(dim=-1, keepdim=True)**0.5 + 1e-9\n",
    "    forces = -1 * g * m1 * m2 / delta_x_norm ** 2\n",
    "\n",
    "    # the delta_x_norms were offset by a small number to avoid numeric problems\n",
    "    # this is fine, when multiplying by delta_x, the self-self terms are zeroed out\n",
    "    force_vectors = forces * delta_x / delta_x_norm\n",
    "    a = force_vectors.sum(dim=2) / m1[:,:,0,:]\n",
    "\n",
    "    X_list.append(torch.cat((pos, vel, m), dim=-1))\n",
    "    y_list.append(a)\n",
    "\n",
    "    # simple 1 step - could use a more intelligent integrator here.\n",
    "    vel += a * dt\n",
    "    pos += vel * dt\n",
    "\n",
    "X = torch.cat(X_list)\n",
    "y = torch.cat(y_list)\n",
    "\n",
    "# remove hidden objects\n",
    "X = X[:,hidden:,:]\n",
    "y = y[:,hidden:,:]\n",
    "\n",
    "# add some random noise\n",
    "y += 0.1 * torch.randn(y.shape) * y.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModule(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(BaseModule, self).__init__()\n",
    "        self.input_size = 3 # r, m1, m2\n",
    "        self.output_size = 1\n",
    "        self.loss = F.mse_loss # torch.log(F.mrse_loss) + angle loss\n",
    "        self.lr = 1e-3\n",
    "        self.wd = 1e-5\n",
    "        # relative mean weighted error - this wasn't helpful at all\n",
    "        # self.loss = lambda y_hat, y: ((y_hat - y).abs() / (y.abs() + 1e-8)).mean()\n",
    "        \n",
    "        self.my_loggers = {\n",
    "            'r_exp': lambda s: s.formula.weight[0][0].item(),\n",
    "            'm1_exp': lambda s: s.formula.weight[0][1].item(),\n",
    "            'm2_exp': lambda s: s.formula.weight[0][2].item()\n",
    "        }\n",
    "        \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_hat = self.forward(X)\n",
    "\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.log('train_loss', loss.item(), on_epoch=True, on_step=False)\n",
    "\n",
    "        # log learning terms\n",
    "        for name, fx in self.my_loggers.items():\n",
    "            self.log(name, fx(self), on_epoch=True, on_step=False)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_hat = self.forward(X)\n",
    "\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.log('validation_loss', loss.item(), on_epoch=True, on_step=False)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.wd)\n",
    "        return optimizer \n",
    "\n",
    "class GnnLogLinearModel(BaseModule):\n",
    "    def __init__(self):\n",
    "        super(GnnLogLinearModel, self).__init__()\n",
    "        self.formula = torch.nn.Linear(self.input_size, self.output_size) \n",
    "\n",
    "    def forward(self, X):\n",
    "        N = X.shape[1]\n",
    "        xs = X[:,:,:d].unsqueeze(2).expand(-1,-1,N,-1)\n",
    "        ms = X[:,:,-1:].unsqueeze(2).expand(-1,-1,N,-1)\n",
    "\n",
    "        x1 = xs\n",
    "        x2 = xs.transpose(1,2)\n",
    "\n",
    "        delta_x = x1 - x2\n",
    "        delta_x_norm = (delta_x ** 2).sum(dim=-1, keepdim=True)**0.5 + 1e-9\n",
    "\n",
    "        m1 = ms\n",
    "        m2 = ms.transpose(1,2)\n",
    "\n",
    "        inp = torch.cat((delta_x_norm, m1, m2), dim=-1)\n",
    "\n",
    "        inp_log = torch.log(inp)\n",
    "\n",
    "        # one linear layer\n",
    "        forces_log = self.formula(inp_log)\n",
    "\n",
    "        forces = torch.exp(forces_log)\n",
    "\n",
    "        # the delta_x_norms were offset by a small number to avoid numeric problems\n",
    "        # this is fine, when multiplying by delta_x, the self-self terms are zeroed out\n",
    "        force_vectors = forces * delta_x / delta_x_norm\n",
    "\n",
    "        # later learn this directionality too (the -1)\n",
    "        return -1 * force_vectors.sum(dim=2) #/ m1[:,:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GnnLogLinearMassModel(BaseModule):\n",
    "    def __init__(self, N=10, formula_given=False):\n",
    "        super(GnnLogLinearMassModel, self).__init__()\n",
    "        #self.formula = torch.nn.Linear(self.input_size, self.output_size) \n",
    "        self.formula = torch.nn.Linear(self.input_size, self.output_size, bias=False) \n",
    "        if formula_given:\n",
    "            #self.formula.weight.requires_grad_(False)\n",
    "            self.formula.weight = torch.nn.Parameter(torch.tensor([[-2.0, 1.0, 1.0]]), requires_grad=False)\n",
    "\n",
    "        # one could be problematic when taking the log, won't set the scale\n",
    "        #self.fixed_mass = torch.nn.Parameter(torch.tensor([[[10.0]]]), requires_grad=False)\n",
    "        #self.other_masses = torch.nn.Parameter(torch.rand(1, N-1, 1), requires_grad=True)\n",
    "        #self.masses = torch.cat((self.fixed_mass, self.other_masses), dim=1).cuda()\n",
    "        self.masses = torch.nn.Parameter(torch.rand(1, N, 1))\n",
    "        #self.masses[0,0,0] = 1.0\n",
    "\n",
    "    def forward(self, X):\n",
    "        N = X.shape[1]\n",
    "        batch_size = X.shape[0]\n",
    "        xs = X[:,:,:d].unsqueeze(2).expand(-1,-1,N,-1)\n",
    "        #ms = X[:,:,-1:].unsqueeze(2).expand(-1,-1,N,-1)\n",
    "        #ms = torch.cat((self.fixed_mass, self.other_masses), dim=1).expand(batch_size,-1,-1).unsqueeze(2).expand(-1,-1,N,-1)\n",
    "        ms = self.masses.expand(batch_size,-1,-1).unsqueeze(2).expand(-1,-1,N,-1)\n",
    "        x1 = xs\n",
    "        x2 = xs.transpose(1,2)\n",
    "\n",
    "        delta_x = x1 - x2\n",
    "        delta_x_norm = (delta_x ** 2).sum(dim=-1, keepdim=True)**0.5 + 1e-9\n",
    "\n",
    "        m1 = ms\n",
    "        m2 = ms.transpose(1,2)\n",
    "\n",
    "        inp = torch.cat((delta_x_norm, m1, m2), dim=-1)\n",
    "\n",
    "        inp_log = torch.log(inp)\n",
    "\n",
    "        # one linear layer\n",
    "        forces_log = self.formula(inp_log)\n",
    "\n",
    "        forces = torch.exp(forces_log)\n",
    "\n",
    "        # the delta_x_norms were offset by a small number to avoid numeric problems\n",
    "        # this is fine, when multiplying by delta_x, the self-self terms are zeroed out\n",
    "        force_vectors = forces * delta_x / delta_x_norm\n",
    "\n",
    "        # later learn this directionality too (the -1)\n",
    "        return -1 * force_vectors.sum(dim=2) / m1[:,:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GnnLogLinearHiddenMassModel(BaseModule):\n",
    "    def __init__(self, N=10, e=1, formula_given=False):\n",
    "        super(GnnLogLinearHiddenMassModel, self).__init__()\n",
    "        self.e = e\n",
    "        self.formula = torch.nn.Linear(self.input_size, self.output_size, bias=False) \n",
    "        if formula_given:\n",
    "            self.formula.weight = torch.nn.Parameter(torch.tensor([[-2.0, 1.0, 1.0]]), requires_grad=False)\n",
    "        \n",
    "        self.masses = torch.nn.Parameter(torch.exp(scale_exp * torch.rand(1, N+e, 1)))\n",
    "        \n",
    "        #self.position = torch.nn.Parameter(torch.rand(1, 1, d) * 0, requires_grad=False)\n",
    "        self.position = torch.nn.Parameter(torch.exp(scale_exp * torch.rand(1, e, d)))\n",
    "\n",
    "        self.my_loggers['pos_norm'] = lambda s: (s.position ** 2).sum() ** 0.5\n",
    "        self.my_loggers['hidden_mass'] = lambda s: s.masses[0][0][0]\n",
    "\n",
    "    def forward(self, X_o):\n",
    "        batch_size = X_o.shape[0]\n",
    "        #X = \n",
    "        X = torch.cat((self.position.expand(batch_size,-1,-1), X_o[:,:,:d]), dim=1)\n",
    "        N = X.shape[1]\n",
    "        \n",
    "        xs = X.unsqueeze(2).expand(-1,-1,N,-1)\n",
    "        #ms = X[:,:,-1:].unsqueeze(2).expand(-1,-1,N,-1)\n",
    "        #ms = torch.cat((self.fixed_mass, self.other_masses), dim=1).expand(batch_size,-1,-1).unsqueeze(2).expand(-1,-1,N,-1)\n",
    "        ms = self.masses.expand(batch_size,-1,-1).unsqueeze(2).expand(-1,-1,N,-1)\n",
    "        x1 = xs\n",
    "        x2 = xs.transpose(1,2)\n",
    "\n",
    "        delta_x = x1 - x2 + 1e-20*(x1+x2)\n",
    "        delta_x_norm = (delta_x ** 2).sum(dim=-1, keepdim=True)**0.5 + 1e-9\n",
    "\n",
    "        m1 = ms\n",
    "        m2 = ms.transpose(1,2)\n",
    "\n",
    "        inp = torch.cat((delta_x_norm, m1, m2), dim=-1)\n",
    "\n",
    "        inp_log = torch.log(inp)\n",
    "\n",
    "        # one linear layer\n",
    "        forces_log = self.formula(inp_log)\n",
    "\n",
    "        forces = torch.exp(forces_log)\n",
    "\n",
    "        # the delta_x_norms were offset by a small number to avoid numeric problems\n",
    "        # this is fine, when multiplying by delta_x, the self-self terms are zeroed out\n",
    "        force_vectors = forces * delta_x / delta_x_norm\n",
    "\n",
    "        # later learn this directionality too (the -1)\n",
    "        acceleration_vectors =  -1 * force_vectors.sum(dim=2) #/ m1[:,:,0,:]\n",
    "        return acceleration_vectors[:,self.e:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GnnLogLinearModelMult(BaseModule):\n",
    "    def __init__(self, mult = 3):\n",
    "        super(GnnLogLinearModelMult, self).__init__()\n",
    "        self.mult = mult\n",
    "        self.formula = torch.nn.Linear(self.input_size * self.mult, self.output_size) \n",
    "\n",
    "    def forward(self, X):\n",
    "        N = X.shape[1]\n",
    "        xs = X[:,:,:d].unsqueeze(2).expand(-1,-1,N,-1)\n",
    "        ms = X[:,:,-1:].unsqueeze(2).expand(-1,-1,N,-1)\n",
    "\n",
    "        x1 = xs\n",
    "        x2 = xs.transpose(1,2)\n",
    "\n",
    "        delta_x = x1 - x2\n",
    "        delta_x_norm = (delta_x ** 2).sum(dim=-1, keepdim=True)**0.5 + 1e-9\n",
    "\n",
    "        m1 = ms\n",
    "        m2 = ms.transpose(1,2)\n",
    "\n",
    "        inp = torch.cat((delta_x_norm, m1, m2), dim=-1)\n",
    "        inp_log = torch.log(inp).repeat(1,1,1,self.mult)\n",
    "\n",
    "        # one linear layer\n",
    "        forces_log = self.formula(inp_log)\n",
    "\n",
    "        forces = torch.exp(forces_log)\n",
    "\n",
    "        # the delta_x_norms were offset by a small number to avoid numeric problems\n",
    "        # this is fine, when multiplying by delta_x, the self-self terms are zeroed out\n",
    "        force_vectors = forces * delta_x / delta_x_norm\n",
    "\n",
    "        # later learn this directionality too (the -1)\n",
    "        return -1 * force_vectors.sum(dim=2) / m1[:,:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GnnLogLinearModelNonLin(BaseModule):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(GnnLogLinearModelNonLin, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.formula = torch.nn.Linear(self.input_size, self.output_size) \n",
    "        self.formula_2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.input_size, self.hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.hidden_size, self.output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        N = X.shape[1]\n",
    "        xs = X[:,:,:d].unsqueeze(2).expand(-1,-1,N,-1)\n",
    "        ms = X[:,:,-1:].unsqueeze(2).expand(-1,-1,N,-1)\n",
    "\n",
    "        x1 = xs\n",
    "        x2 = xs.transpose(1,2)\n",
    "\n",
    "        delta_x = x1 - x2\n",
    "        delta_x_norm = (delta_x ** 2).sum(dim=-1, keepdim=True)**0.5 + 1e-9\n",
    "\n",
    "        m1 = ms\n",
    "        m2 = ms.transpose(1,2)\n",
    "\n",
    "        inp = torch.cat((delta_x_norm, m1, m2), dim=-1)\n",
    "        inp_log = torch.log(inp)\n",
    "\n",
    "        # one linear layer\n",
    "        forces_log = self.formula(inp_log)\n",
    "        forces_log_2 = self.formula_2(inp_log)\n",
    "\n",
    "        forces = torch.exp(forces_log + forces_log_2)\n",
    "\n",
    "        # the delta_x_norms were offset by a small number to avoid numeric problems\n",
    "        # this is fine, when multiplying by delta_x, the self-self terms are zeroed out\n",
    "        force_vectors = forces * delta_x / delta_x_norm\n",
    "\n",
    "        # later learn this directionality too (the -1)\n",
    "        return -1 * force_vectors.sum(dim=2) / m1[:,:,0,:]\n",
    "\n",
    "    # see get_parameters for more complex configurations: https://stackoverflow.com/questions/69217682/what-is-the-best-way-to-define-adam-optimizer-in-pytorch\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam([\n",
    "                {'params': self.formula.parameters()},\n",
    "                {'params': self.formula_2.parameters(), 'weight_decay': 1e-4}\n",
    "            ], lr=self.lr)\n",
    "        return optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "for mult in [1,3,10,20]:\n",
    "\n",
    "    train_set = list(zip(X, y))\n",
    "    train_set_size = int(len(train_set) * 0.8)\n",
    "    valid_set_size = len(train_set) - train_set_size\n",
    "    train_set, valid_set = random_split(train_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "\n",
    "    best_model = None\n",
    "    best_score = 1e15\n",
    "    times = 5\n",
    "\n",
    "    for _ in range(times):\n",
    "        #model = GnnLogLinearModelMult(mult=mult)\n",
    "        model = GnnLogLinearModelNonLin(hidden_size=mult)\n",
    "        #model = GnnLogLinearMassModel(10)\n",
    "        #model = GnnLogLinearHiddenMassModel(10, e=10, formula_given=False)\n",
    "        y_hat = model.forward(X)\n",
    "        loss = model.loss(y_hat, y)\n",
    "        if loss < best_score:\n",
    "            print(loss)\n",
    "            best_score = loss\n",
    "            best_model = model\n",
    "\n",
    "\n",
    "    model = best_model\n",
    "    early_stop_callback = EarlyStopping(monitor=\"validation_loss\", patience=30, verbose=False, mode=\"min\")\n",
    "\n",
    "    train_set = DataLoader(train_set, shuffle=True, batch_size=128)\n",
    "    valid_set = DataLoader(valid_set, shuffle=True, batch_size=1000)\n",
    "\n",
    "    logger = TensorBoardLogger(\"lightning_logs\", name=f'gnn_log_linear_dec_hidden{mult}') # _masses, hidden_multiple\n",
    "\n",
    "    # train with both splits\n",
    "    trainer = pl.Trainer(gpus=1, max_epochs=10000,\n",
    "                                #gradient_clip_val=0.5,\n",
    "                                callbacks=[early_stop_callback],\n",
    "                                logger=logger,\n",
    "                                enable_progress_bar=False)\n",
    "\n",
    "    trainer.fit(model, train_set, valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "early_stop_callback = EarlyStopping(monitor=\"validation_loss\", patience=300, verbose=False, mode=\"min\")\n",
    "\n",
    "train_set = DataLoader(train_set, shuffle=True, batch_size=128)\n",
    "valid_set = DataLoader(valid_set, shuffle=True, batch_size=1000)\n",
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name='gnn_log_linear_mult1') # _masses, hidden_multiple\n",
    "\n",
    "# train with both splits\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=10000,\n",
    "                            #gradient_clip_val=0.5,\n",
    "                            callbacks=[early_stop_callback],\n",
    "                            logger=logger)\n",
    "\n",
    "trainer.fit(model, train_set, valid_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[40.4088],\n",
       "         [ 2.9725],\n",
       "         [ 5.6881],\n",
       "         [ 8.0561],\n",
       "         [32.0705],\n",
       "         [ 2.0293],\n",
       "         [ 1.0138],\n",
       "         [59.0399],\n",
       "         [60.7772],\n",
       "         [ 3.3891]]),\n",
       " tensor([[[39.0646],\n",
       "          [ 2.9494],\n",
       "          [ 5.6877],\n",
       "          [ 8.0410],\n",
       "          [31.9989],\n",
       "          [ 2.0037],\n",
       "          [ 0.9688],\n",
       "          [58.7323],\n",
       "          [60.7730],\n",
       "          [ 3.3401]]], device='cuda:0', grad_fn=<PowBackward1>))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare fitted masses to ground truth\n",
    "g * m[0],  model.masses ** model.formula.weight[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to run tensorboard in the terminal:\n",
    "# tensorboard --logdir lightning_logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "4741426e26a5e6a637207f4863e4b645de3b3c5f81c70cde841ac5e1e8af37aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
