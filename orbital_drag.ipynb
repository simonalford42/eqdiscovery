{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zsomk\\anaconda3\\envs\\torch_pl\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One pair of (X,y) looks like:\n",
    "- d = 3 # number of dimensions\n",
    "- N # number of objects\n",
    "- X: N x (2 * d + 1) # positions, velocities, mass\n",
    "- y: N x d # accelerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048 * 32 # 16\n",
    "visible = 10\n",
    "hidden = 0\n",
    "N = visible + hidden\n",
    "d = 3\n",
    "\n",
    "scale_exp = 5\n",
    "\n",
    "pos = torch.exp(scale_exp * torch.rand(batch_size, N, d))\n",
    "# make it centered at 0\n",
    "pos -= pos.mean(axis=1, keepdim = True) \n",
    "\n",
    "vel = torch.exp(scale_exp * torch.rand(batch_size, N, d))\n",
    "\n",
    "# assign fixed positions, velocities??? (this shouldn't matter for now) to hidden objects (this only works for one that is put in the center for now)\n",
    "pos[:,:hidden,:] *= 0\n",
    "vel[:,:hidden,:] *= 0\n",
    "\n",
    "m = torch.rand(1, N, 1)\n",
    "# hidden mass:\n",
    "m[0,:hidden,0] = m[0,:hidden,0] * 0 + 1\n",
    "\n",
    "m = torch.exp(scale_exp * m)\n",
    "m = m.expand(batch_size,-1,-1)\n",
    "\n",
    "dt = 0.01\n",
    "g = 0.5\n",
    "drag_exp = 1.2\n",
    "drag_const = 0.1 #100 #0.1\n",
    "\n",
    "ms = m.unsqueeze(2).expand(-1,-1,N,-1)\n",
    "m1 = ms\n",
    "m2 = ms.transpose(1,2)\n",
    "\n",
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "for _ in range(1):\n",
    "    xs = pos.unsqueeze(2).expand(-1,-1,N,-1)\n",
    "    x1 = xs\n",
    "    x2 = xs.transpose(1,2)\n",
    "\n",
    "    delta_x = x1 - x2\n",
    "    delta_x_norm = (delta_x ** 2).sum(dim=-1, keepdim=True)**0.5 + 1e-9\n",
    "    forces = -1 * g * m1 * m2 / delta_x_norm ** 2\n",
    "\n",
    "    # the delta_x_norms were offset by a small number to avoid numeric problems\n",
    "    # this is fine, when multiplying by delta_x, the self-self terms are zeroed out\n",
    "    force_vectors = forces * delta_x / delta_x_norm\n",
    "    \n",
    "    vel_norm = (vel ** 2).sum(dim=-1, keepdim=True) ** 0.5 + 1e-9\n",
    "    drag_force = -1 * drag_const * vel * vel_norm ** (drag_exp - 1)\n",
    "    a = (force_vectors.sum(dim=2) + drag_force) / m1[:,:,0,:]\n",
    "\n",
    "    X_list.append(torch.cat((pos, vel, m), dim=-1))\n",
    "    y_list.append(a)\n",
    "\n",
    "    # simple 1 step - could use a more intelligent integrator here.\n",
    "    vel += a * dt\n",
    "    pos += vel * dt\n",
    "\n",
    "X = torch.cat(X_list)\n",
    "y = torch.cat(y_list)\n",
    "\n",
    "# remove hidden objects\n",
    "X = X[:,hidden:,:]\n",
    "y = y[:,hidden:,:]\n",
    "\n",
    "# add some random noise\n",
    "noise = 0.00\n",
    "y *= (1 + noise * torch.randn(y.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModule(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(BaseModule, self).__init__()\n",
    "        self.input_size = 3 # r, m1, m2\n",
    "        self.output_size = 1\n",
    "        self.loss = F.mse_loss # torch.log(F.mrse_loss) + angle loss\n",
    "        self.lr = 3e-4\n",
    "        self.wd = 0 #1e-5\n",
    "        # relative mean weighted error - this wasn't helpful at all\n",
    "        # self.loss = lambda y_hat, y: ((y_hat - y).abs() / (y.abs() + 1e-8)).mean()\n",
    "        \n",
    "        self.my_loggers = {\n",
    "            'r_exp': lambda s: s.formula.weight[0][0].item(),\n",
    "            'm1_exp': lambda s: s.formula.weight[0][1].item(),\n",
    "            'm2_exp': lambda s: s.formula.weight[0][2].item()\n",
    "        }\n",
    "        \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_hat = self.forward(X)\n",
    "\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.log('train_loss', loss.item(), on_epoch=True, on_step=False)\n",
    "\n",
    "        # log learning terms\n",
    "        for name, fx in self.my_loggers.items():\n",
    "            self.log(name, fx(self), on_epoch=True, on_step=False)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_hat = self.forward(X)\n",
    "\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.log('validation_loss', loss.item(), on_epoch=True, on_step=False)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.wd)\n",
    "        return optimizer \n",
    "\n",
    "class GnnLogLinearModel(BaseModule):\n",
    "    def __init__(self):\n",
    "        super(GnnLogLinearModel, self).__init__()\n",
    "        self.formula = torch.nn.Linear(self.input_size + 1, self.output_size) \n",
    "        self.drag_const = torch.nn.Parameter(torch.Tensor([1.0]))\n",
    "        self.drag_exp = torch.nn.Parameter(torch.Tensor([1.0]))\n",
    "        self.drag_formula = torch.nn.Linear(3, 1, bias=True) \n",
    "\n",
    "    def forward(self, X):\n",
    "        N = X.shape[1]\n",
    "        x = X[:,:,:d]\n",
    "        xs = x.unsqueeze(2).expand(-1,-1,N,-1)\n",
    "        vel = X[:,:,d:2*d]\n",
    "        m = X[:,:,-1:]\n",
    "        ms = m.unsqueeze(2).expand(-1,-1,N,-1)\n",
    "\n",
    "        x1 = xs\n",
    "        x2 = xs.transpose(1,2)\n",
    "\n",
    "        delta_x = x1 - x2\n",
    "        delta_x_norm = (delta_x ** 2).sum(dim=-1, keepdim=True)**0.5 + 1e-9\n",
    "        x_norm = (x ** 2).sum(dim=-1, keepdim=True)**0.5 + 1e-9\n",
    "\n",
    "        m1 = ms\n",
    "        m2 = ms.transpose(1,2)\n",
    "\n",
    "        vel_norm = (vel ** 2).sum(dim=-1, keepdim=True) ** 0.5 + 1e-9\n",
    "\n",
    "        inp = torch.cat((delta_x_norm, m1, m2, vel_norm.unsqueeze(2).expand(-1,-1,N,-1)), dim=-1)\n",
    "\n",
    "        inp_log = torch.log(inp)\n",
    "\n",
    "        # one linear layer\n",
    "        forces_log = self.formula(inp_log)\n",
    "\n",
    "        forces = torch.exp(forces_log)\n",
    "\n",
    "        # the delta_x_norms were offset by a small number to avoid numeric problems\n",
    "        # this is fine, when multiplying by delta_x, the self-self terms are zeroed out\n",
    "        force_vectors = -1 * forces * delta_x / delta_x_norm\n",
    "\n",
    "        \n",
    "\n",
    "        inp_drag = torch.cat((x_norm, m, vel_norm), dim=-1)\n",
    "\n",
    "        drag_force = -1 *  (vel / vel_norm) * torch.exp(self.drag_formula(torch.log(inp_drag)))\n",
    "        #drag_force = -1 * self.drag_const * vel * vel_norm ** (self.drag_exp - 1)\n",
    "        a = (force_vectors.sum(dim=2) + drag_force) / m1[:,:,0,:]\n",
    "\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GnnLogLinearMassModel(BaseModule):\n",
    "    def __init__(self, N=10, formula_given=False):\n",
    "        super(GnnLogLinearMassModel, self).__init__()\n",
    "        #self.formula = torch.nn.Linear(self.input_size, self.output_size) \n",
    "        #self.formula = torch.nn.Linear(self.input_size, self.output_size, bias=False) \n",
    "        #self.drag_formula = torch.nn.Linear(1, 1, bias=True) \n",
    "\n",
    "        self.formula = torch.nn.Linear(self.input_size + 1, self.output_size) \n",
    "        self.drag_formula = torch.nn.Linear(3, 1, bias=True) \n",
    "\n",
    "        #self.drag_const = torch.nn.Parameter(torch.Tensor([1.0]))\n",
    "        #self.drag_exp = torch.nn.Parameter(torch.Tensor([1.0]))\n",
    "        if formula_given:\n",
    "            #self.formula.weight.requires_grad_(False)\n",
    "            self.formula.weight = torch.nn.Parameter(torch.tensor([[-2.0, 1.0, 1.0]]), requires_grad=False)\n",
    "\n",
    "        # one could be problematic when taking the log, won't set the scale\n",
    "        #self.fixed_mass = torch.nn.Parameter(torch.tensor([[[10.0]]]), requires_grad=False)\n",
    "        #self.other_masses = torch.nn.Parameter(torch.rand(1, N-1, 1), requires_grad=True)\n",
    "        #self.masses = torch.cat((self.fixed_mass, self.other_masses), dim=1).cuda()\n",
    "        self.masses = torch.nn.Parameter(torch.rand(1, N, 1))\n",
    "        #self.masses[0,0,0] = 1.0\n",
    "\n",
    "    def forward(self, X):\n",
    "        N = X.shape[1]\n",
    "        batch_size = X.shape[0]\n",
    "        x = X[:,:,:d]\n",
    "        xs = x.unsqueeze(2).expand(-1,-1,N,-1)\n",
    "        vel = X[:,:,d:2*d]\n",
    "        #ms = X[:,:,-1:].unsqueeze(2).expand(-1,-1,N,-1)\n",
    "        #ms = torch.cat((self.fixed_mass, self.other_masses), dim=1).expand(batch_size,-1,-1).unsqueeze(2).expand(-1,-1,N,-1)\n",
    "        m = self.masses.expand(batch_size,-1,-1)\n",
    "        ms = m.unsqueeze(2).expand(-1,-1,N,-1)\n",
    "        x1 = xs\n",
    "        x2 = xs.transpose(1,2)\n",
    "\n",
    "        delta_x = x1 - x2\n",
    "        delta_x_norm = (delta_x ** 2).sum(dim=-1, keepdim=True)**0.5 + 1e-9\n",
    "        x_norm = (x ** 2).sum(dim=-1, keepdim=True)**0.5 + 1e-9\n",
    "\n",
    "        m1 = ms\n",
    "        m2 = ms.transpose(1,2)\n",
    "\n",
    "        vel_norm = (vel ** 2).sum(dim=-1, keepdim=True) ** 0.5 + 1e-9\n",
    "\n",
    "        inp = torch.cat((delta_x_norm, m1, m2, vel_norm.unsqueeze(2).expand(-1,-1,N,-1)), dim=-1)\n",
    "\n",
    "        inp_log = torch.log(inp)\n",
    "\n",
    "        # one linear layer\n",
    "        forces_log = self.formula(inp_log)\n",
    "\n",
    "        forces = torch.exp(forces_log)\n",
    "\n",
    "        # the delta_x_norms were offset by a small number to avoid numeric problems\n",
    "        # this is fine, when multiplying by delta_x, the self-self terms are zeroed out\n",
    "        force_vectors = -1 * forces * delta_x / delta_x_norm\n",
    "\n",
    "        \n",
    "\n",
    "        inp_drag = torch.cat((x_norm, m, vel_norm), dim=-1)\n",
    "\n",
    "        drag_force = -1 *  (vel / vel_norm) * torch.exp(self.drag_formula(torch.log(inp_drag)))\n",
    "        #drag_force = -1 * self.drag_const * vel * vel_norm ** (self.drag_exp - 1)\n",
    "        a = (force_vectors.sum(dim=2) + drag_force) / m1[:,:,0,:]\n",
    "\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GnnLogLinearHiddenMassModel(BaseModule):\n",
    "    def __init__(self, N=10, e=1, formula_given=False):\n",
    "        super(GnnLogLinearHiddenMassModel, self).__init__()\n",
    "        self.e = e\n",
    "        self.formula = torch.nn.Linear(self.input_size, self.output_size, bias=False) \n",
    "        if formula_given:\n",
    "            self.formula.weight = torch.nn.Parameter(torch.tensor([[-2.0, 1.0, 1.0]]), requires_grad=False)\n",
    "        \n",
    "        self.masses = torch.nn.Parameter(torch.exp(scale_exp * torch.rand(1, N+e, 1)))\n",
    "        \n",
    "        #self.position = torch.nn.Parameter(torch.rand(1, 1, d) * 0, requires_grad=False)\n",
    "        self.position = torch.nn.Parameter(torch.exp(scale_exp * torch.rand(1, e, d)))\n",
    "\n",
    "        self.my_loggers['pos_norm'] = lambda s: (s.position ** 2).sum() ** 0.5\n",
    "        self.my_loggers['hidden_mass'] = lambda s: s.masses[0][0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GnnLogLinearModelMult(BaseModule):\n",
    "    def __init__(self, mult = 3):\n",
    "        super(GnnLogLinearModelMult, self).__init__()\n",
    "        self.mult = mult\n",
    "        self.formula = torch.nn.Linear(self.input_size * self.mult, self.output_size) \n",
    "\n",
    "    def forward(self, X):\n",
    "        N = X.shape[1]\n",
    "        xs = X[:,:,:d].unsqueeze(2).expand(-1,-1,N,-1)\n",
    "        ms = X[:,:,-1:].unsqueeze(2).expand(-1,-1,N,-1)\n",
    "\n",
    "        x1 = xs\n",
    "        x2 = xs.transpose(1,2)\n",
    "\n",
    "        delta_x = x1 - x2\n",
    "        delta_x_norm = (delta_x ** 2).sum(dim=-1, keepdim=True)**0.5 + 1e-9\n",
    "\n",
    "        m1 = ms\n",
    "        m2 = ms.transpose(1,2)\n",
    "\n",
    "        inp = torch.cat((delta_x_norm, m1, m2), dim=-1)\n",
    "        inp_log = torch.log(inp).repeat(1,1,1,self.mult)\n",
    "\n",
    "        # one linear layer\n",
    "        forces_log = self.formula(inp_log)\n",
    "\n",
    "        forces = torch.exp(forces_log)\n",
    "\n",
    "        # the delta_x_norms were offset by a small number to avoid numeric problems\n",
    "        # this is fine, when multiplying by delta_x, the self-self terms are zeroed out\n",
    "        force_vectors = forces * delta_x / delta_x_norm\n",
    "\n",
    "        # later learn this directionality too (the -1)\n",
    "        return -1 * force_vectors.sum(dim=2) / m1[:,:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GnnLogLinearModelNonLin(BaseModule):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(GnnLogLinearModelNonLin, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.formula = torch.nn.Linear(self.input_size, self.output_size) \n",
    "        self.formula_2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.input_size, self.hidden_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.hidden_size, self.output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        N = X.shape[1]\n",
    "        xs = X[:,:,:d].unsqueeze(2).expand(-1,-1,N,-1)\n",
    "        ms = X[:,:,-1:].unsqueeze(2).expand(-1,-1,N,-1)\n",
    "\n",
    "        x1 = xs\n",
    "        x2 = xs.transpose(1,2)\n",
    "\n",
    "        delta_x = x1 - x2\n",
    "        delta_x_norm = (delta_x ** 2).sum(dim=-1, keepdim=True)**0.5 + 1e-9\n",
    "\n",
    "        m1 = ms\n",
    "        m2 = ms.transpose(1,2)\n",
    "\n",
    "        inp = torch.cat((delta_x_norm, m1, m2), dim=-1)\n",
    "        inp_log = torch.log(inp)\n",
    "\n",
    "        # one linear layer\n",
    "        forces_log = self.formula(inp_log)\n",
    "        forces_log_2 = self.formula_2(inp_log)\n",
    "\n",
    "        forces = torch.exp(forces_log + forces_log_2)\n",
    "\n",
    "        # the delta_x_norms were offset by a small number to avoid numeric problems\n",
    "        # this is fine, when multiplying by delta_x, the self-self terms are zeroed out\n",
    "        force_vectors = forces * delta_x / delta_x_norm\n",
    "\n",
    "        # later learn this directionality too (the -1)\n",
    "        return -1 * force_vectors.sum(dim=2) / m1[:,:,0,:]\n",
    "\n",
    "    # see get_parameters for more complex configurations: https://stackoverflow.com/questions/69217682/what-is-the-best-way-to-define-adam-optimizer-in-pytorch\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam([\n",
    "                {'params': self.formula.parameters()},\n",
    "                {'params': self.formula_2.parameters(), 'weight_decay': 1e-4}\n",
    "            ], lr=self.lr)\n",
    "        return optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1179.6294, grad_fn=<MseLossBackward0>)\n",
      "tensor(976.8427, grad_fn=<MseLossBackward0>)\n",
      "tensor(128.6735, grad_fn=<MseLossBackward0>)\n",
      "tensor(118.3015, grad_fn=<MseLossBackward0>)\n",
      "tensor(24.9110, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: lightning_logs\\gnn_log_linear_masses_01drag_nonoise_nowdec_form_ext\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type   | Params\n",
      "----------------------------------------\n",
      "0 | formula      | Linear | 5     \n",
      "1 | drag_formula | Linear | 4     \n",
      "----------------------------------------\n",
      "19        Trainable params\n",
      "0         Non-trainable params\n",
      "19        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "for mult in [1]:\n",
    "\n",
    "    train_set = list(zip(X, y))\n",
    "    train_set_size = int(len(train_set) * 0.8)\n",
    "    valid_set_size = len(train_set) - train_set_size\n",
    "    train_set, valid_set = random_split(train_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "\n",
    "    best_model = None\n",
    "    best_score = 1e15\n",
    "    times = 20\n",
    "\n",
    "    for _ in range(times):\n",
    "        #model = GnnLogLinearModelMult(mult=mult)\n",
    "        #model = GnnLogLinearModel()\n",
    "        model = GnnLogLinearMassModel(10)\n",
    "        #model = GnnLogLinearHiddenMassModel(10, e=10, formula_given=False)\n",
    "        y_hat = model.forward(X)\n",
    "        loss = model.loss(y_hat, y)\n",
    "        if loss < best_score:\n",
    "            print(loss)\n",
    "            best_score = loss\n",
    "            best_model = model\n",
    "\n",
    "\n",
    "    model = best_model\n",
    "    early_stop_callback = EarlyStopping(monitor=\"validation_loss\", patience=500, verbose=False, mode=\"min\")\n",
    "\n",
    "    train_set = DataLoader(train_set, shuffle=True, batch_size=256)\n",
    "    valid_set = DataLoader(valid_set, shuffle=True, batch_size=1000)\n",
    "\n",
    "    logger = TensorBoardLogger(\"lightning_logs\", name=f'gnn_log_linear_masses_01drag_nonoise_nowdec_form_ext') # _masses, hidden_multiple\n",
    "\n",
    "    # train with both splits\n",
    "    trainer = pl.Trainer(gpus=1, max_epochs=10000,\n",
    "                                #gradient_clip_val=0.5,\n",
    "                                callbacks=[early_stop_callback],\n",
    "                                logger=logger,\n",
    "                                enable_progress_bar=False)\n",
    "\n",
    "    trainer.fit(model, train_set, valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 8.0517e-05, -1.0160e+00,  1.2002e+00]], requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.drag_formula.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay, very intersting, cause this way it is possible to have the exact same results. All masses are the sqrt of the originals. \n",
    "# And then here we are dividing with the square of the mass in practice for drag.\n",
    "# maybe really doing library learning is the only way to fix this (and/or including additional symmetries to the force laws?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.9978e+00,  9.9700e-01,  2.0122e+00, -8.0673e-04]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.formula.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([2.0003], device='cuda:0', requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.1001], device='cuda:0', requires_grad=True))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.drag_exp, model.drag_const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type   | Params\n",
      "----------------------------------------\n",
      "0 | formula      | Linear | 5     \n",
      "1 | drag_formula | Linear | 4     \n",
      "----------------------------------------\n",
      "19        Trainable params\n",
      "0         Non-trainable params\n",
      "19        Total params\n",
      "0.000     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 442:  80%|███████▉  | 349/437 [54:44<13:48,  9.41s/it, loss=0.157, v_num=1]     "
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "early_stop_callback = EarlyStopping(monitor=\"validation_loss\", patience=300, verbose=False, mode=\"min\")\n",
    "\n",
    "#train_set = DataLoader(train_set, shuffle=True, batch_size=128)\n",
    "#valid_set = DataLoader(valid_set, shuffle=True, batch_size=1000)\n",
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name='gnn_log_linear_masses_01drag_nowdec_form_ext') # _masses, hidden_multiple\n",
    "\n",
    "# train with both splits\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=10000,\n",
    "                            #gradient_clip_val=0.5,\n",
    "                            callbacks=[early_stop_callback],\n",
    "                            logger=logger)\n",
    "\n",
    "trainer.fit(model, train_set, valid_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[40.4088],\n",
       "         [ 2.9725],\n",
       "         [ 5.6881],\n",
       "         [ 8.0561],\n",
       "         [32.0705],\n",
       "         [ 2.0293],\n",
       "         [ 1.0138],\n",
       "         [59.0399],\n",
       "         [60.7772],\n",
       "         [ 3.3891]]),\n",
       " tensor([[[39.0646],\n",
       "          [ 2.9494],\n",
       "          [ 5.6877],\n",
       "          [ 8.0410],\n",
       "          [31.9989],\n",
       "          [ 2.0037],\n",
       "          [ 0.9688],\n",
       "          [58.7323],\n",
       "          [60.7730],\n",
       "          [ 3.3401]]], device='cuda:0', grad_fn=<PowBackward1>))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare fitted masses to ground truth\n",
    "g * m[0],  model.masses ** model.formula.weight[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to run tensorboard in the terminal:\n",
    "# tensorboard --logdir lightning_logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('torch_pl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4741426e26a5e6a637207f4863e4b645de3b3c5f81c70cde841ac5e1e8af37aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
